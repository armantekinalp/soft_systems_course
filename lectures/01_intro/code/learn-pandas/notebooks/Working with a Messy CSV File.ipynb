{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ---------------------------------------------------- -->\n",
    "  <div class=\"col-sm-3 col-md-3 col-lg-3\">\n",
    "\t<!-- logo -->  \n",
    "    <div class=\"img-responsive\">\n",
    "      <img src=\"https://www.dropbox.com/s/220ncn0o5danuey/pandas-ipython-tutorials-hedaro.jpg?dl=1\" title=\"Pandas Tutorial | Hedaro\" alt=\"Pandas Tutorial | Hedaro\">    \n",
    "    </div>\n",
    "\t<!-- logo -->\t\n",
    "  </div>\n",
    "<!-- ---------------------------------------------------- --> \n",
    "  <div class=\"col-sm-6 col-md-6 col-lg-6\">\n",
    "\t<!-- Pandas Tutorial -->  \n",
    "\t  <center>\n",
    "\t    <br>\n",
    "        <h1>Working with a Messy CSV File</h1>\n",
    "        <p>Here is a CSV file that is an accumulation of seperate test results. The goal is to reorganize \n",
    "           the data so an actual analysis is possible.</p>\n",
    "\t  </center>\t\n",
    "    <!-- Pandas Tutorial -->\t\n",
    "  </div>\n",
    "<!-- ---------------------------------------------------- --> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>The most popular Pandas tutorials. </strong><a href=\"https://gumroad.com/l/jVeRh\" target=\"_blank\"><strong>Get a total of 7 tutorials!</strong></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Python version ' + sys.version)\n",
    "print('Pandas version ' + pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open the file from Hell\n",
    "\n",
    "I first just opened the file and noticed how the different chunks of data were separated. From the output below, you will see ***\\n,,,,,,,,\\n*** and this is what we will use as our delimiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.csv', 'r') as myfile:\n",
    "    print(myfile.read().split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we separate the different chunks of data and place them in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file and split\n",
    "with open ('test.csv', 'r') as myfile:\n",
    "    data = myfile.read().split('\\n,,,,,,,,\\n')\n",
    "\n",
    "# show me the first chunk\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use StringIO(x)?  \n",
    "\n",
    "Since pd.read_csv() needs an actual csv file as input, we fake it using StringIO.  \n",
    "\n",
    "* In cases where you want a file-like object that ***ACTS*** like a file, but is writing to an in-memory string buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert each chunk we have parsed thus far and turn it into a dataframe\n",
    "pieces = [pd.read_csv(StringIO(x),sep=',') for x in data]\n",
    "\n",
    "# get first df\n",
    "pieces[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn the list of dataframes into one big dataframe  \n",
    "\n",
    "[Click Here](http://www.hedaro.com/pandas-concat) if you are unfamilar with the ***Concat*** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat(pieces)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Pandas magic to shape the data just how we want it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(merged.set_index('Wavelength').unstack()).reset_index()\n",
    "\n",
    "# we rename the columns as Pandas gives them generic names\n",
    "df.columns = ['index', 'Wavelength', 'values']\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now on to the index CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you can see, we basically used the same stradegy here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('index.csv', 'r') as myfile:\n",
    "    data = myfile.read().split('\\n,,,,,,,,,\\n')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pieces = [pd.read_csv(StringIO(x),sep=',') for x in data]\n",
    "\n",
    "# We do get a bunch on Unnamed columns\n",
    "pieces[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we then glue all the pieces and make one big dataframe\n",
    "merged2 = pd.concat(pieces, axis=1)\n",
    "merged2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We rename the columns here because in the next sections, we are going to take advantage of the different groups having identical column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged2 = merged2.rename(columns = {'strain':'index', 'col':'index'})\n",
    "\n",
    "# Each of the three groups will have identical column names\n",
    "merged2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is the way I figured out how to reshape the data. You might find there is a beter/easier way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all the column names\n",
    "col = merged2.columns.unique()\n",
    "\n",
    "pieces = []\n",
    "\n",
    "# pick a column name and stick it in a list\n",
    "# note every iteration will have a df of three columns\n",
    "for c in col:\n",
    "    n = merged2[c]\n",
    "    \n",
    "    # We make every chunk has the same column names\n",
    "    # We do this so that when we \"concat\", we end up with three columns\n",
    "    n.columns = ['index','strain','col']\n",
    "    pieces.append(n)\n",
    "    \n",
    "pieces[0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all of the pieces into one big dataframe\n",
    "lookup = pd.concat(pieces) \n",
    "lookup.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then join on the \"index\" columns on both dataframes\n",
    "df.merge(lookup, how='left', left_on='index', right_on='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All of this could have been avoided if each chunk/piece of data was saved into a separate csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<p class=\"text-muted\">This tutorial was created by <a href=\"http://www.hedaro.com\" target=\"_blank\"><strong>HEDARO</strong></a></p>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
