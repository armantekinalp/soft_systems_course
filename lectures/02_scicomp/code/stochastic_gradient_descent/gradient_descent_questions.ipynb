{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivative-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thanks and Credits\n",
    "The core exercises are taken directly from [Daniel Newman's Github repository](https://github.com/dtnewman/stochastic_gradient_descent), which is distributed freely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import fmin\n",
    "plt.style.use('seaborn-white')\n",
    "plt.rcParams.update({'font.size': 18})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Gradient descent</b>, also known as <b>steepest descent</b>, is an optimization algorithm for finding the local minimum of a function. To find a local minimum, the function \"steps\" in the  direction of the negative of the gradient. <b>Gradient ascent</b> is the same as gradient descent, except that it steps in the direction of the positive of the gradient and therefore finds local maximums instead of minimums. The algorithm of gradient descent can be outlined as follows:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; 1: &nbsp; Choose initial guess $x_0$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    2: &nbsp; <b>for</b> k = 0, 1, 2, ... <b>do</b> <br>\n",
    "&nbsp;&nbsp;&nbsp;    3:   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $s_k$ = -$\\nabla f(x_k)$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    4:   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; choose $\\eta_k$ to minimize $f(x_k+\\eta_k s_k)$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    5:   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $x_{k+1} = x_k + \\eta_k s_k$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    6: &nbsp;  <b>end for</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple example, let's find a local minimum for the function $f(x) = x^3-2x^2+2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x) : \n",
    "    # Fill in the blanks\n",
    "    # Takes in a single floating point number, spits out the function value\n",
    "    return 0 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates an array from -1 to 2.5 with 1000 points in between\n",
    "x = np.linspace(-1,2.5,1000)\n",
    "\n",
    "# Familiarize yourself with this syntax for matplotlib. \n",
    "# It should be pretty intutive the more you see it.\n",
    "plt.plot(x, f(x))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.xlim([-1,2.5])\n",
    "plt.ylim([0,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from plot above that our local minimum is gonna be near around 1.4 or 1.5 (on the x-axis), but let's pretend that we don't know that, so we set our starting point (arbitrarily, in this case) at $x_0 = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_old = 0\n",
    "x_new = 2 # The algorithm starts at x=2\n",
    "n_k = 0.1 # step size parameter eta above\n",
    "precision = 0.0001 # some desired precision so that we can stop iterating\n",
    "\n",
    "# lists that can append values, used for plotting later\n",
    "x_list, y_list = [x_new], [f(x_new)]\n",
    "\n",
    "# returns the value of the derivative of our function\n",
    "def f_prime(x):\n",
    "    # fill in f_prime\n",
    "    # Takes in a numpy array, spits out the function value\n",
    "    return 0.0 * x\n",
    "\n",
    "# Fill in the algorithm\n",
    "# Fill in criterion : iterate till values are \"sufficiently\" close to one another\n",
    "while 0 > 1:\n",
    "    # Remove this pass statement\n",
    "    pass\n",
    "    # Do calculation as per the formula above here\n",
    "    \n",
    "    # Append values to list here ...\n",
    "\n",
    "    \n",
    "print(\"Local minimum occurs at:\", x_new)\n",
    "print(\"Number of steps:\", len(x_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figures below show the route that was taken to find the local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,3])\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(x,f(x))\n",
    "plt.plot(x_list,y_list,\"ro-\", ms=12)\n",
    "plt.xlim([-1,2.5])\n",
    "plt.ylim([0,3])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title(\"Gradient descent\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(x,f(x))\n",
    "plt.plot(x_list,y_list,\"ro-\", ms=12)\n",
    "plt.xlim([1.2,2.1])\n",
    "plt.ylim([0,3])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title(\"Gradient descent (zoomed in)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the step size (also called learning rate) in the implementation above is constant, unlike the algorithm in the pseudocode. Doing this makes it easier to implement the algorithm. However, it also presents some issues: If the step size is too small, then convergence will be very slow, but if we make it too large, then the method may fail to converge at all. \n",
    "\n",
    "A solution to this is to use adaptive step sizes as the algorithm below does (using `scipy`'s `fmin` function to find optimal step sizes). I will showcase this in-class only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach to update the step size is choosing a decrease constant $d$ that shrinks the step size over time:\n",
    "$\\eta(t+1) = \\eta(t) / (1+t \\times d)$. This is commonly done in supervised machine-learning methods (where a variation of steepest descent called the Stochastic Gradient Descent (SGD) is used).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_old = 0\n",
    "x_new = 2 # The algorithm starts at x=2\n",
    "n_k = 0.17 # step size\n",
    "precision = 0.0001\n",
    "t, d = 0, 1\n",
    "\n",
    "x_list, y_list = [x_new], [f(x_new)]\n",
    "\n",
    "# returns the value of the derivative of our function\n",
    "def f_prime(x):\n",
    "    # fill in f_prime or use one filled above\n",
    "    return 0.0 * x\n",
    "\n",
    "# Fill in the algorithm\n",
    "# Fill in criterion : iterate till values are \"sufficiently\" close to one another\n",
    "while 0 > 1:\n",
    "    # Remove this pass statement\n",
    "    pass\n",
    "    # Do calculation here\n",
    "    \n",
    "    # Adapt eta here\n",
    "    \n",
    "    # Append to list here ..\n",
    "\n",
    "\n",
    "print(\"Local minimum occurs at:\", x_new)\n",
    "print(\"Number of steps:\", len(x_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent in two-dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same algorithm works independent of the dimensions! The derivatives are now gradients and hence vectors...\n",
    "\n",
    "Let's work on finding the minimum of a function $ x^2 + \\texttt{stretch_factor}*y^2 $ where `stretch_factor` is a variable that can be changed, using the constant step-size version of steppest-descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_old = np.array([0.0, 0.0])\n",
    "x_new = np.array([6.0, 6.0]) # The algorithm starts at x=6.0,6.0\n",
    "n_k = 0.1 # step size\n",
    "precision = 0.0001\n",
    "t, d = 0, 1\n",
    "\n",
    "# controls how the contour plot is stretched in the x/y-direction\n",
    "stretch_factor = 1.0\n",
    "\n",
    "def f(x):\n",
    "    # fill in, takes in an array of size (2,) and spits out a single number\n",
    "    return 0.0 * x[0] + 0.0 * x[1]\n",
    "\n",
    "# returns the value of the derivative of our function\n",
    "def f_prime(x):\n",
    "    # fill in \n",
    "    # Takes in an array of size (2,) and spits out an array of (2,)\n",
    "    return 0.0 * x\n",
    "\n",
    "# lists that can append values, used for plotting later\n",
    "x_list, y_list = [x_new], [f(x_new)]\n",
    "\n",
    "# Fill in criterion : iterate till values are \"sufficiently\" close to one another\n",
    "while 0 > 1:\n",
    "    # Fill in algorithm\n",
    "\n",
    "    # You should see that with numpy you can essentially write\n",
    "    # code that 'looks' like you are operating on a single number\n",
    "    # but you are doing array operations!\n",
    "    \n",
    "    # lists that can append values, used for plotting later\n",
    "    pass\n",
    "\n",
    "print(\"Local minimum occurs at:\", x_new)\n",
    "print(\"Number of steps:\", len(x_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "x_collection = np.array(x_list)\n",
    "x_collection = x_collection if x_collection.shape[1] == 2 else x_collection.T\n",
    "ax.plot(x_collection[:, 0], x_collection[:, 1], 'ro-', ms=14)\n",
    "grid_x = np.linspace(-6.0, 6.0, 100)\n",
    "grid_y = np.linspace(-6.0, 6.0, 100)\n",
    "X,Y = np.meshgrid(grid_x, grid_y)\n",
    "Z = f([X, Y])\n",
    "ax.contourf(X, Y ,Z, cmap=plt.cm.viridis)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('f(x,y)')\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brittle\n",
    "But it's very easy to break. Try changing the `stretch_factor` in the example above (we started of from 1, how about changing it to 2,4,8,16...)?. \n",
    "\n",
    "\n",
    "The conjugate gradient method overcomes this _difficulty_ with `stretch_factor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method of Conjugate Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need to minimize a function of the form\n",
    "\n",
    "$$ \\mathbf{x}^* = \\textrm{argmin} \\left( {\\tfrac {1}{2}} \\mathbf{x}^{\\mathsf {T}} \\mathbf{A} \\mathbf{x} - \\mathbf{x}^{\\mathsf {T}}\\mathbf{b} \\right) $$\n",
    "\n",
    "which reduces to solving $ \\mathbf{A} \\mathbf{x} - \\mathbf{b} = 0$, we can use the following algorithm (found [here](https://en.wikipedia.org/wiki/Conjugate_gradient_method#The_resulting_algorithm)). An approachable introduction to understand CG can be found in this [link](http://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}&\\mathbf {r} _{0}:=\\mathbf {b} -\\mathbf {Ax} _{0}\\\\&{\\hbox{if }}\\mathbf {r} _{0}{\\text{ is sufficiently small, then return }}\\mathbf {x} _{0}{\\text{ as the result}}\\\\&\\mathbf {p} _{0}:=\\mathbf {r} _{0}\\\\&k:=0\\\\&{\\text{repeat}}\\\\&\\qquad \\alpha _{k}:={\\frac {\\mathbf {r} _{k}^{\\mathsf {T}}\\mathbf {r} _{k}}{\\mathbf {p} _{k}^{\\mathsf {T}}\\mathbf {Ap} _{k}}}\\\\&\\qquad \\mathbf {x} _{k+1}:=\\mathbf {x} _{k}+\\alpha _{k}\\mathbf {p} _{k}\\\\&\\qquad \\mathbf {r} _{k+1}:=\\mathbf {r} _{k}-\\alpha _{k}\\mathbf {Ap} _{k}\\\\&\\qquad {\\hbox{if }}\\mathbf {r} _{k+1}{\\text{ is sufficiently small, then exit loop}}\\\\&\\qquad \\beta _{k}:={\\frac {\\mathbf {r} _{k+1}^{\\mathsf {T}}\\mathbf {r} _{k+1}}{\\mathbf {r} _{k}^{\\mathsf {T}}\\mathbf {r} _{k}}}\\\\&\\qquad \\mathbf {p} _{k+1}:=\\mathbf {r} _{k+1}+\\beta _{k}\\mathbf {p} _{k}\\\\&\\qquad k:=k+1\\\\&{\\text{end repeat}}\\\\&{\\text{return }}\\mathbf {x} _{k+1}{\\text{ as the result}}\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can couch the problem seen before, of minimizing $x^2 + \\texttt{stretch_factor} * y^2$ into the following form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\mathbf{x}^* = \\textrm{argmin} \\left( {\\tfrac {1}{2}} \\mathbf{x}^{\\mathsf {T}} \\cdot \\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "0 & \\texttt{stretch_factor}\n",
    "\\end{bmatrix}\n",
    "\\cdot \\mathbf{x} - \\mathbf{x}^{\\mathsf {T}}\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\\right) \\\\\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stretch_factor = 100.0\n",
    "A = np.random.randn(2,2) # What do you think A should be? \n",
    "b = np.random.randn(2,)  # What do you think b should be? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial guess value which solves the problem\n",
    "x = np.array([6.0, 6.0])\n",
    "x_list = [x]\n",
    "\n",
    "# Optional : use a \"max\" number of iterations beyond which the simulation\n",
    "# doesn't run\n",
    "i = 0\n",
    "imax = 10 \n",
    "\n",
    "# Tolerance\n",
    "eps = 0.0001\n",
    "\n",
    "# Start algorithm here\n",
    "# Do some initial setup before the repeat block above\n",
    "\n",
    "# initial setup\n",
    "\n",
    "# Setup conditions for the loop\n",
    "while 0 > 1 and 1  > 2:\n",
    "    # Complex processing\n",
    "\n",
    "    # Loop counter\n",
    "    i += 1\n",
    "    \n",
    "    # Don't forget to append data to list!\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "x_collection = np.array(x_list)\n",
    "x_collection = x_collection if x_collection.shape[1] == 2 else x_collection.T\n",
    "ax.plot(x_collection[:, 0], x_collection[:, 1], 'ro-', ms=14)\n",
    "grid_x = np.linspace(-6.0, 6.0, 100)\n",
    "grid_y = np.linspace(-6.0, 6.0, 100)\n",
    "X,Y = np.meshgrid(grid_x, grid_y)\n",
    "Z = f([X, Y])\n",
    "ax.contourf(X, Y ,Z, cmap=plt.cm.viridis)\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is this realistic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's great, but how useful is it in real-life functions that are\n",
    "- Multi-modal (the above was a unimodal function, with one global minima)\n",
    "- Non-convex (the above was a convex function)\n",
    "- Non-separable (in the above example x and y are equivalent but separate)\n",
    "- Non-linear (the above problem is essentially linear)\n",
    "\n",
    "? \n",
    "\n",
    "To test that, let's take the Rastrigin function that was discussed a couple of lectures ago and apply steepest descent and CG to minimize it. We need to locally linearize the problem at every step, which involves finding gradients (first-derivatives : a vector) and Hessians (second-derivatives : a matrix) of the function! The rastrigin function in two dimensions is :\n",
    "$$f(\\mathbf{x}) = 20 + \\left[ x^2 - 10 \\cos\\left(2 \\pi x \\right) \\right] + \\left[ y^2 - 10 \\cos\\left(2 \\pi y \\right) \\right]$$\n",
    "\n",
    "For this part, I'll demonstrate the math and code in class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
