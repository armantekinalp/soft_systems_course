{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivative-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thanks and Credits\n",
    "The core exercises are taken directly from [Daniel Newman's Github repository](https://github.com/dtnewman/stochastic_gradient_descent), which is distributed freely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import fmin\n",
    "plt.style.use('seaborn-white')\n",
    "plt.rcParams.update({'font.size': 18})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Gradient descent</b>, also known as <b>steepest descent</b>, is an optimization algorithm for finding the local minimum of a function. To find a local minimum, the function \"steps\" in the  direction of the negative of the gradient. <b>Gradient ascent</b> is the same as gradient descent, except that it steps in the direction of the positive of the gradient and therefore finds local maximums instead of minimums. The algorithm of gradient descent can be outlined as follows:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; 1: &nbsp; Choose initial guess $x_0$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    2: &nbsp; <b>for</b> k = 0, 1, 2, ... <b>do</b> <br>\n",
    "&nbsp;&nbsp;&nbsp;    3:   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $s_k$ = -$\\nabla f(x_k)$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    4:   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; choose $\\alpha_k$ to minimize $f(x_k+\\alpha_k s_k)$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    5:   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $x_{k+1} = x_k + \\alpha_k s_k$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    6: &nbsp;  <b>end for</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple example, let's find a local minimum for the function $f(x) = x^3-2x^2+2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x) : return x**3 - 2.0*x**2 + 2.0\n",
    "# An alternate way of doing the same thing:\n",
    "# f = lambda x: x**3-2*x**2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1,2.5,1000)\n",
    "plt.plot(x, f(x))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.xlim([-1,2.5])\n",
    "plt.ylim([0,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from plot above that our local minimum is gonna be near around 1.4 or 1.5 (on the x-axis), but let's pretend that we don't know that, so we set our starting point (arbitrarily, in this case) at $x_0 = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_old = 0\n",
    "x_new = 2 # The algorithm starts at x=2\n",
    "n_k = 0.1 # step size\n",
    "precision = 0.0001\n",
    "\n",
    "x_list, y_list = [x_new], [f(x_new)]\n",
    "\n",
    "# returns the value of the derivative of our function\n",
    "def f_prime(x):\n",
    "    return 3*x**2-4*x\n",
    " \n",
    "while abs(x_new - x_old) > precision:\n",
    "    x_old = x_new\n",
    "    s_k = -f_prime(x_old)\n",
    "    x_new = x_old + n_k * s_k\n",
    "    x_list.append(x_new)\n",
    "    y_list.append(f(x_new))\n",
    "print(\"Local minimum occurs at:\", x_new)\n",
    "print(\"Number of steps:\", len(x_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figures below show the route that was taken to find the local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,3])\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(x,f(x))\n",
    "plt.plot(x_list,y_list,\"ro-\", ms=12)\n",
    "plt.xlim([-1,2.5])\n",
    "plt.ylim([0,3])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title(\"Gradient descent\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(x,f(x))\n",
    "plt.plot(x_list,y_list,\"ro-\", ms=12)\n",
    "plt.xlim([1.2,2.1])\n",
    "plt.ylim([0,3])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title(\"Gradient descent (zoomed in)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the step size (also called learning rate) in the implementation above is constant, unlike the algorithm in the pseudocode. Doing this makes it easier to implement the algorithm. However, it also presents some issues: If the step size is too small, then convergence will be very slow, but if we make it too large, then the method may fail to converge at all. \n",
    "\n",
    "A solution to this is to use adaptive step sizes as the algorithm below does (using `scipy`'s `fmin` function to find optimal step sizes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we setup this function to pass into the fmin algorithm\n",
    "def f2(n,x,s):\n",
    "    x = x + n*s\n",
    "    return f(x)\n",
    "\n",
    "x_old = 0\n",
    "x_new = 2 # The algorithm starts at x=2\n",
    "precision = 0.0001\n",
    "\n",
    "x_list, y_list = [x_new], [f(x_new)]\n",
    "\n",
    "# returns the value of the derivative of our function\n",
    "def f_prime(x):\n",
    "    return 3*x**2-4*x\n",
    "\n",
    "while abs(x_new - x_old) > precision:\n",
    "    x_old = x_new\n",
    "    s_k = -f_prime(x_old)\n",
    "    \n",
    "    # use scipy fmin function to find ideal step size.\n",
    "    n_k = fmin(f2,0.1,(x_old,s_k), full_output = False, disp = False)\n",
    "\n",
    "    x_new = x_old + n_k * s_k\n",
    "    x_list.append(x_new)\n",
    "    y_list.append(f(x_new))\n",
    "    \n",
    "print(\"Local minimum occurs at \", float(x_new))\n",
    "print(\"Number of steps:\", len(x_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With adaptive step sizes, the algorithm converges in just 4 iterations rather than 17. Of course, it takes time to compute the appropriate step size at each iteration. Here are some plots of the path taken below. You can see that it converges very quickly to a point near the local minimum, so it's hard to even discern the dots after the first two steps until we zoom in very close in the third frame below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,3])\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(x,f(x))\n",
    "plt.plot(x_list,y_list,\"ro-\", ms=12)\n",
    "plt.xlim([-1,2.5])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title(\"Gradient descent\")\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(x,f(x))\n",
    "plt.plot(x_list,y_list,\"ro-\", ms=12)\n",
    "plt.xlim([1.2,2.1])\n",
    "plt.ylim([0,3])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title(\"zoomed in\")\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(x,f(x))\n",
    "plt.plot(x_list,y_list,\"ro-\", ms=12)\n",
    "plt.xlim([1.333,1.334])\n",
    "plt.ylim([0.814,0.816])\n",
    "plt.xlabel('x')\n",
    "#plt.ylabel('f(x)')\n",
    "plt.title(\"zoomed in more\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach to update the step size is choosing a decrease constant $d$ that shrinks the step size over time:\n",
    "$\\eta(t+1) = \\eta(t) / (1+t \\times d)$. This is commonly done in supervised machine-learning methods (where a variation of steepest descent called the Stochastic Gradient Descent (SGD) is used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_old = 0\n",
    "x_new = 2 # The algorithm starts at x=2\n",
    "n_k = 0.17 # step size\n",
    "precision = 0.0001\n",
    "t, d = 0, 1\n",
    "\n",
    "x_list, y_list = [x_new], [f(x_new)]\n",
    "\n",
    "# returns the value of the derivative of our function\n",
    "def f_prime(x):\n",
    "    return 3*x**2-4*x\n",
    " \n",
    "while abs(x_new - x_old) > precision:\n",
    "    x_old = x_new\n",
    "    s_k = -f_prime(x_old)\n",
    "    x_new = x_old + n_k * s_k\n",
    "    x_list.append(x_new)\n",
    "    y_list.append(f(x_new))\n",
    "    n_k = n_k / (1 + t * d)\n",
    "    t += 1\n",
    "\n",
    "print(\"Local minimum occurs at:\", x_new)\n",
    "print(\"Number of steps:\", len(x_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent in two-dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same algorithm works independent of the dimensions! The derivatives are now gradients and hence vectors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_old = np.array([0.0, 0.0])\n",
    "x_new = np.array([6.0, 6.0]) # The algorithm starts at x=2\n",
    "n_k = 0.1 # step size\n",
    "precision = 0.0001\n",
    "t, d = 0, 1\n",
    "\n",
    "stretch_factor = 10\n",
    "def f(x):\n",
    "    return x[0]**2 + stretch_factor * x[1]**2\n",
    "\n",
    "# returns the value of the derivative of our function\n",
    "def f_prime(x):\n",
    "    return np.array([2.0*x[0], 2.0*stretch_factor*x[1]])\n",
    "\n",
    "def f2(n,x,s):\n",
    "    x = x + n*s\n",
    "    return f(x)\n",
    "\n",
    "x_list, y_list = [x_new], [f(x_new)]\n",
    "\n",
    "while np.linalg.norm(x_new - x_old) > precision:\n",
    "    x_old = x_new\n",
    "    s_k = -f_prime(x_old)\n",
    "    # use scipy fmin function to find ideal step size.\n",
    "    # n_k = fmin(f2,0.1,(x_old,s_k), full_output = False, disp = False)\n",
    "    x_new = x_old + n_k * s_k\n",
    "    x_list.append(x_new)\n",
    "    y_list.append(f(x_new))\n",
    "    #n_k = n_k / (1 + t * d)\n",
    "    #t += 1\n",
    "\n",
    "print(\"Local minimum occurs at:\", x_new)\n",
    "print(\"Number of steps:\", len(x_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "x_collection = np.array(x_list)\n",
    "x_collection = x_collection if x_collection.shape[1] == 2 else x_collection.T\n",
    "ax.plot(x_collection[:, 0], x_collection[:, 1], 'ro-', ms=14)\n",
    "grid_x = np.linspace(-6.0, 6.0, 100)\n",
    "grid_y = np.linspace(-6.0, 6.0, 100)\n",
    "X,Y = np.meshgrid(grid_x, grid_y)\n",
    "Z = f([X, Y])\n",
    "ax.contourf(X, Y ,Z, cmap=plt.cm.viridis)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('f(x,y)')\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brittle\n",
    "But it's very easy to break. Try changing the `stretch_factor` in the example above. The conjugate gradient method overcomes this difficulty with `stretch_factor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method of Conjugate Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need to minimize a function of the form\n",
    "\n",
    "$$ \\mathbf{x}^* = \\textrm{argmin} \\left( {\\tfrac {1}{2}} \\mathbf{x}^{\\mathsf {T}} \\mathbf{A} \\mathbf{x} - \\mathbf{x}^{\\mathsf {T}}\\mathbf{b} \\right) $$\n",
    "\n",
    "which reduces to solving $ \\mathbf{A} \\mathbf{x} - \\mathbf{b} = 0$, we can use the following algorithm (found [here](https://en.wikipedia.org/wiki/Conjugate_gradient_method#The_resulting_algorithm)). An approachable introduction to understand CG can be found in this [link](http://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}&\\mathbf {r} _{0}:=\\mathbf {b} -\\mathbf {Ax} _{0}\\\\&{\\hbox{if }}\\mathbf {r} _{0}{\\text{ is sufficiently small, then return }}\\mathbf {x} _{0}{\\text{ as the result}}\\\\&\\mathbf {p} _{0}:=\\mathbf {r} _{0}\\\\&k:=0\\\\&{\\text{repeat}}\\\\&\\qquad \\alpha _{k}:={\\frac {\\mathbf {r} _{k}^{\\mathsf {T}}\\mathbf {r} _{k}}{\\mathbf {p} _{k}^{\\mathsf {T}}\\mathbf {Ap} _{k}}}\\\\&\\qquad \\mathbf {x} _{k+1}:=\\mathbf {x} _{k}+\\alpha _{k}\\mathbf {p} _{k}\\\\&\\qquad \\mathbf {r} _{k+1}:=\\mathbf {r} _{k}-\\alpha _{k}\\mathbf {Ap} _{k}\\\\&\\qquad {\\hbox{if }}\\mathbf {r} _{k+1}{\\text{ is sufficiently small, then exit loop}}\\\\&\\qquad \\beta _{k}:={\\frac {\\mathbf {r} _{k+1}^{\\mathsf {T}}\\mathbf {r} _{k+1}}{\\mathbf {r} _{k}^{\\mathsf {T}}\\mathbf {r} _{k}}}\\\\&\\qquad \\mathbf {p} _{k+1}:=\\mathbf {r} _{k+1}+\\beta _{k}\\mathbf {p} _{k}\\\\&\\qquad k:=k+1\\\\&{\\text{end repeat}}\\\\&{\\text{return }}\\mathbf {x} _{k+1}{\\text{ as the result}}\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can couch the problems seen above, of minimizing $x^2 + \\texttt{stretch_factor} * y^2$ into the following form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\mathbf{x}^* = \\textrm{argmin} \\left( {\\tfrac {1}{2}} \\mathbf{x}^{\\mathsf {T}} \\cdot \\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "0 & \\texttt{stretch_factor}\n",
    "\\end{bmatrix}\n",
    "\\cdot \\mathbf{x} - \\mathbf{x}^{\\mathsf {T}}\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\\right) \\\\\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stretch_factor = 100.0\n",
    "A = np.array([[1.0, 0.0], [0.0, stretch_factor]])\n",
    "b = np.zeros((2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([6.0, 6.0])\n",
    "x_list = [x]\n",
    "i = 0\n",
    "imax = 10 # max number of iterations\n",
    "eps = 0.0001\n",
    "r = b - A@x\n",
    "d = r\n",
    "deltanew = np.inner(r, r)\n",
    "delta0 = deltanew\n",
    "while i < imax and deltanew > eps**2 * delta0:\n",
    "    alpha = float(deltanew / np.inner(d , (A @ d)))\n",
    "    x = x + alpha * d\n",
    "    x_list.append(x)\n",
    "    r = b - A @ x\n",
    "    deltaold = deltanew\n",
    "    deltanew = np.inner(r, r)\n",
    "    #beta = -float((r.T * A * d) / float(d.T * A * d))\n",
    "    beta = float(deltanew / float(deltaold))\n",
    "    d = r + beta * d\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "x_collection = np.array(x_list)\n",
    "x_collection = x_collection if x_collection.shape[1] == 2 else x_collection.T\n",
    "ax.plot(x_collection[:, 0], x_collection[:, 1], 'ro-', ms=14)\n",
    "grid_x = np.linspace(-6.0, 6.0, 100)\n",
    "grid_y = np.linspace(-6.0, 6.0, 100)\n",
    "X,Y = np.meshgrid(grid_x, grid_y)\n",
    "Z = f([X, Y])\n",
    "ax.contourf(X, Y ,Z, cmap=plt.cm.viridis)\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is this realistic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's great, but how useful is it in real-life functions that are\n",
    "- Multi-modal (the above was a unimodal function, with one global minima)\n",
    "- Non-convex (the above was a convex function)\n",
    "- Non-separable (in the above example x and y are equivalent but separate)\n",
    "- Non-linear (the above problem is essentially linear)\n",
    "\n",
    "? \n",
    "\n",
    "To test that, let's take the Rastrigin function that was discussed a couple of lectures ago and apply steepest descent and CG to minimize it. We need to locally linearize the problem at every step, which involves finding gradients (first-derivatives : a vector) and Hessians (second-derivatives : a matrix) of the function! The rastrigin function in two dimensions is :\n",
    "$$f(\\mathbf{x}) = 20 + \\left[ x^2 - 10 \\cos\\left(2 \\pi x \\right) \\right] + \\left[ y^2 - 10 \\cos\\left(2 \\pi y \\right) \\right]$$\n",
    "\n",
    "The gradient is :\n",
    "$$ \\nabla f(\\mathbf{x}) = \\begin{bmatrix}\n",
    "2x + 20 \\pi \\sin\\left(2 \\pi x \\right) \\\\\n",
    "2y + 20 \\pi \\sin\\left(2 \\pi y \\right) \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and finally the Hessian \n",
    "$$ \\nabla^2 f(\\mathbf{x}) = \\begin{bmatrix}\n",
    "2 + 40 \\pi^2 \\cos\\left(2 \\pi x \\right) &  0\\\\\n",
    "0 & 2 + 40 \\pi^2 \\cos\\left(2 \\pi x \\right)\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([4.0, 3.0])\n",
    "x_list = [x]\n",
    "\n",
    "i = 0\n",
    "imax = 10 # max number of iterations\n",
    "eps = 0.0001\n",
    "\n",
    "def f(x):\n",
    "    return 20.0 + ((x[0]-2.0)**2 - 10.0 * np.cos(2.0 * np.pi * (x[0]-2.0))) + ((x[1]-2.0)**2 - 10.0 * np.cos(2.0 * np.pi * (x[1]-2.0)))\n",
    "\n",
    "def grad_f(x):\n",
    "    return np.array([2.0 * (x[0]-2.0) + 20.0 * np.pi * np.sin(2.0 * np.pi * (x[0]-2.0)),\n",
    "                     2.0 * (x[1]-2.0) + 20.0 * np.pi * np.sin(2.0 * np.pi * (x[1]-2.0))])\n",
    "\n",
    "def hessian(x):\n",
    "    return np.array([[2.0 + 40.0 * np.pi**2 * np.cos(2.0 * np.pi * (x[0]-2.0)), 0.0],\n",
    "                     [0.0, 2.0 + 40.0 * np.pi**2 * np.cos(2.0 * np.pi * (x[1]-2.0))]])\n",
    "\n",
    "\n",
    "r = grad_f(x) - hessian(x)@x\n",
    "d = r\n",
    "deltanew = np.inner(r, r)\n",
    "delta0 = deltanew\n",
    "\n",
    "while i < imax and deltanew > eps**2 * delta0:\n",
    "    alpha = float(deltanew / np.inner(d , (A @ d)))\n",
    "    x = x + alpha * d\n",
    "    A = hessian(x)\n",
    "    b = grad_f(x)\n",
    "    x_list.append(x)\n",
    "    r = b - A @ x\n",
    "    deltaold = deltanew\n",
    "    deltanew = np.inner(r, r)\n",
    "    beta = float(deltanew / float(deltaold))\n",
    "    d = r + beta * d\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "x_collection = np.array(x_list)\n",
    "ax.plot(x_collection[:, 0], x_collection[:, 1], 'ro-', ms=14)\n",
    "grid_x = np.linspace(-5.0, 5.0, 100)\n",
    "grid_y = np.linspace(-5.0, 5.0, 100)\n",
    "X,Y = np.meshgrid(grid_x, grid_y)\n",
    "Z = f([X, Y])\n",
    "ax.contourf(X, Y ,Z, cmap=plt.cm.viridis)\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
