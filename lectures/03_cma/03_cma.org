#+TITLE: Covariance Matrix Adaptation in Python
#+AUTHOR: /Tejaswin Parthasarathy/, Mattia Gazzola
#+SUBTITLE: ME498: Comp. modeling & optimization
#+BEAMER_FRAME_LEVEL: 2
# #+BEAMER_HEADER: \institute[INST]{Institute\\\url{http://www.institute.edu}}
# #+BEAMER_HEADER: \titlegraphic{\includegraphics[height=1.5cm]{test}}

#+STARTUP: beamer
#+LATEX_CLASS: beamer
# #+LATEX_CLASS_OPTIONS: [presentation]
#+LATEX_CLASS_OPTIONS: [notes]
#+LATEX_HEADER:\usetheme[progressbar=frametitle]{metropolis}
#+LATEX_HEADER:\newcommand{\gv}[1]{\ensuremath{\mbox{\boldmath$ #1 $}}}
#+LATEX_HEADER:\newcommand{\bv}[1]{\ensuremath{\mathbf{#1}}}
#+LATEX_HEADER:\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
#+LATEX_HEADER:\newcommand{\order}[1]{\mathcal O \left( #1 \right)} % order of magnitude
#+LATEX_HEADER:\definecolor{scarlet}{rgb}{1.0, 0.13, 0.0}
#+LATEX_HEADER:\definecolor{shamrockgreen}{rgb}{0.0, 0.62, 0.38}
#+LATEX_HEADER:\definecolor{royalblue}{rgb}{0.25, 0.41, 0.88}
#+OPTIONS:   H:2 num:t toc:nil ::t |:t ^:{} -:t f:t *:t <:t
#+OPTIONS:   tex:t d:nil todo:t pri:nil tags:nil
#+COLUMNS: %45ITEM %10BEAMER_ENV(Env) %10BEAMER_ACT(Act) %4BEAMER_COL(Col) %8BEAMER_OPT(Opt)
* ~Matplotlib~
** Additional packages
  - ~Seaborn~  *DEMO*
  - ~bokeh~
  - ~plotly~
  - See [[https://wiki.python.org/moin/NumericAndScientific/Plotting][Python wiki]] for more plotting tools
** Quick review
*** GA demo : Results of our GA implementation
*** How to improve convergence?
	+ Parameters?
	+ Strategies? (More mutation, less recombination say...?)
	+ Tuning is painful
*** \Rightarrow CMAes (and other algorithms)
* Implementation of CMAes
** TODO The algorithm
** Starting CMAes
*** Problem independent
   - Set evolution paths \( \gv{p}_\sigma = \gv{0}, \gv{p}_c = \gv{0}\)
   - Set number of generations \( g = 0 \)
   - Set covariance matrix \( \bv{C} = \bv{I} \) (Why?)
*** Problem dependent
   - Distribution mean \( \gv{m} \in \mathbb{R}^n \)
   - Step size \( \sigma \in \mathbb{R}_{>0} \) (Important to set \( >0\) )
   - Population size \( n \)
** Starting CMAes: more on problem dependent parameters
   - Optimum presumably be within the initial cube \( \gv{m} \pm 3 \sigma
     \left(1 ,1 , \cdots, 1 \right)^T\)
   - \( \therefore \) if optima \( \in [a, b]^{n} \) choose \( \gv{m} \in [a,b]^{n}
     \) (as a uniformly random vector) and \( \sigma = 0.3*(b-a) \)
   - Different search intervals \( \Delta s_i \) for different variables can be
     done using \( \bv{C} \) as shown below (deferred discussion):

   #+NAME: lyap_asym
   \begin{equation}
   \begin{aligned}
   \begin{bmatrix}
   \Delta s^2_1 & 0 & \cdots & 0 \\
   0 & \Delta s^2_2 & \cdots & 0 \\
   \vdots & \ddots & \ddots & \vdots \\
   0 & 0 &  \cdots & \Delta s^n_2  \\
   \end{bmatrix}
   \end{aligned}
   \end{equation}

   - \( \Delta s_i \) all must be of similar magnitude (for conditioning). Else,
     rescale your problem.
** First step : Sampling
*** New population of points, for \( k = 1 \cdots \lambda \)
   - \( \gv{y}_k \sim \mathcal{N}\left( \gv{0}, \bv{I} \right) \)
   - \( \gv{z}_k \sim \mathcal{N}\left( \gv{0}, \bv{C} \right) =
     \bv{B}\bv{D}\gv{y}_k \)
	 - Given \( \bv{C} = \bv{B}\bv{D}^2\bv{B}^T \)
	 - Consult [fn:1]  for a proof of why \( \bv{A} \mathcal{N}\left(\gv{0}, \bv{I}
       \right) = \mathcal{N} \left(\gv{0}, \bv{A}\bv{A}^T \right) \)
   - \( \gv{x}_k = \gv{m} + \sigma \gv{z}_k \sim \mathcal{N}\left( \gv{m},
     \sigma^2 \bv{C} \right) \)
*** Computing?
   - Steps 1 and 3 above using ~*/np.multiply~ for
     elementwise multiplication and ~+~ for elementwise addition
*** We need a way to sample correlated (across dimensions) populations from ~numpy~ :B_ignoreheading:
	:PROPERTIES:
	:BEAMER_env: ignoreheading
	:END:
   *We need a way to sample correlated (across dimensions) populations from* ~numpy~
** Sampling : Python
*** How to sample a multivariate normal distribution?
	- ~np.random.multivariate_normal~ *DEMO*
*** Caveats?
	- What is \( \bv{C}\) / ~cov~ (in a 2D case) and its meaning?
	- ~cov~ needs to be SPD. Is it? What about the update step?
	- What happens in ~numpy~ if it is not?
***                                                                  :B_note:
	:PROPERTIES:
	:BEAMER_env: note
	:END:
	- \( \mu_x = \frac{1}{N} \sum_{i=1}^{N}x_i \)
	- \( \sigma_x^2 = \frac{1}{N} \sum_{i=1}^{N}(x_i - \mu_x)^2 \) and \(
      \sigma_{xy} = \frac{1}{N} \sum_{i=1}^{N}(x_i - \mu_x)(y_i - \mu_y) \)
	- Show symmetric nature of ~cov~ after update
	- Show positive definiteness (use PD of C, as well as the fact that it is
	  made up of rank-one decompositions
	- ~numpy~ checks for PD and throws an error
** Covariance Matrix [fn:4]
  #+begin_export latex
  \includegraphics[page=68,width=1.0\textwidth]{images/cma_slideshare.pdf}
  #+end_export
** Covariance Matrix : Example 1
  #+begin_export latex
  \includegraphics[page=69,width=1.0\textwidth]{images/cma_slideshare.pdf}
  #+end_export
** Covariance Matrix : Example 2
  #+begin_export latex
  \includegraphics[page=70,width=1.0\textwidth]{images/cma_slideshare.pdf}
  #+end_export
** Covariance Matrix : Example 3
  #+begin_export latex
  \includegraphics[page=71,width=1.0\textwidth]{images/cma_slideshare.pdf}
  #+end_export
** Covariance Matrix : Example 4
  #+begin_export latex
  \includegraphics[page=72,width=1.0\textwidth]{images/cma_slideshare.pdf}
  #+end_export
** Covariance Matrix : Example 5
  #+begin_export latex
  \includegraphics[page=73,width=1.0\textwidth]{images/cma_slideshare.pdf}
  #+end_export
** Covariance Matrix : Definition
  #+begin_export latex
  \includegraphics[page=74,width=1.0\textwidth]{images/cma_slideshare.pdf}
  #+end_export
** Covariance Matrix : Definition
  #+begin_export latex
  \includegraphics[page=75,width=1.0\textwidth]{images/cma_slideshare.pdf}
  #+end_export
** Covariance Matrix : Example
  #+begin_export latex
  \includegraphics[page=76,width=1.0\textwidth]{images/cma_slideshare.pdf}
  #+end_export
** Covariance Matrix : Definition
  #+begin_export latex
  \includegraphics[page=77,width=1.0\textwidth]{images/cma_slideshare.pdf}
  #+end_export
** Covariance Matrix : Example
  #+begin_export latex
  \includegraphics[page=78,width=1.0\textwidth]{images/cma_slideshare.pdf}
  #+end_export
** Sampling : Python--Answers
	* What is \( \bv{C}\) / ~cov~ (in a 2D case) and its meaning?
	  1. Covariance, how a gene varies with another (across dimensions)
	  2. \( \mu_x = \frac{1}{N} \sum_{i=1}^{N}x_i \)
	  3. \( \sigma_x^2 = \frac{1}{N} \sum_{i=1}^{N}(x_i - \mu_x)^2 \) and \(
		 \sigma_{xy} = \frac{1}{N} \sum_{i=1}^{N}(x_i - \mu_x)(y_i - \mu_y) \)
	* ~cov~ needs to be SPD. Is it? What about the update step?
	  1. Symmetric by definition
	  2. Symmetric after update too
	* What happens in ~numpy~ if it is not?
	  1. ~numpy~ checks for PD, else throws an exception

** PCA
*** CMAes performs PCA on the optimzation data
*** PCA?
	1) Principal Component Analysis
	2) Find directions with
	   - High Variance
	   - Low Covariance with other components
	3) Find dimensions that are "independent" from one another
	4) Gives basis (in this case for \( \bv{C}\) )
** Sampling : Idea of \( \bv{C} \) \rightarrow math
*** What is CMA-ES doing?
	- How does CMA estimate \( \bv{C} \)?
	- What about the choice of weights?
	- What is CMA doing by adapting \( \bv{C}\)?
***                                                                  :B_note:
	:PROPERTIES:
	:BEAMER_env: note
	:END:
	- \( \mu_x^{(g+1)} = \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}x_i \) and \(
      \sigma_x^{2, (g+1)} = \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}(x_i -
      \mu_x^{(g)})^2 \)
	- This is rank \( \mu \) update, but with also exponential weighting of
      previous \( C \) (show separately, will be discussed in CMA)
	- Choice of weights reflect "normalization"
	- Conducts PCA (eigenvectors), rotated representation \( \bv{C} =
	  \bv{B}\bv{D}^2\bv{D}^T \), inverse Hessian
	  (second order)
** Sampling : Idea of \( \bv{C} \) \rightarrow math--Answers
	- How does CMA estimate \( \bv{C} \)?
	  1. You can use the new population to get \( \bv{C} \) too, but information
         is lost (no information on how the population "evolved")
	  2. *Idea* : Use \(  \mu_x^{(g+1)} = \frac{1}{N_{best}}
         \sum_{i=1}^{N_{best}}x_i \) rather than \(\sigma_x^{2, (g+1)} =
         \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}(x_i - \mu_x^{(g)})^2 \),
         across \( N_{best}\) individuals to estimate covariance between genes
         (rank \( \mu \) update)
	  3. Exponential weighting, discussed later on
	- What about the choice of weights?
	  1. Reflect normalization (relates back to the ability of CMA to maintain invariance)
	- What is CMA doing by adapting \( \bv{C}\)?
	  1. Conducts PCA (eigenvectors), rotated representation \( \bv{C} =
		 \bv{B}\bv{D}^2\bv{D}^T \), inverse Hessian
		 (second order)
** Sampling : Parameters
*** Choice of \( \lambda \)?
	- Look at the CMA tutorial : [[file:~/Desktop/Masters_Resources/readings/optimization/Hansen/The%20CMA%20evolution%20strategy%20A%20tutorial.pdf][The CMA tutorial]]/[[https://arxiv.org/pdf/1604.00772.pdf][CMA tutorial on Arxiv]]
	- Usually \( \lambda = \lfloor 4 + 3 \ln n \rfloor \)
	- And \( \mu = \lfloor \lambda/ 2 \rfloor \)
** Second step : Selection
*** How to select \( \mu \) best individuals
   - \( \langle \gv{z}_k \rangle_{w} = \sum_{i=1}^{\mu} w_i \gv{z}_{i:\lambda} \)
   - Constraint on weights: \( \sum_{i=1}^{\mu} w_i = 1, \; w_i > 0 \; \forall i=1
     \cdots \mu \) (at least in our version of CMA)
*** Computing?
   - Fitness function evaluation left upto user (including constraints etc.).
     This determines the \( \mu \) best individuals.
   - The weighted sum can be evaluated using ~np.inner()/broadcasting with
     */np.sum() after *~ ...
** Selection : Parameters
*** Choice of \( w_i \)?
	- Look at the CMA tutorial : [[file:~/Desktop/Masters_Resources/readings/optimization/Hansen/The%20CMA%20evolution%20strategy%20A%20tutorial.pdf][The CMA tutorial]]/[[https://arxiv.org/pdf/1604.00772.pdf][CMA tutorial on Arxiv]]
** Third step : Recombination
*** Recombination to get new \( m \)
   - \( \gv{m} \leftarrow \gv{m} + \sigma \langle \gv{z} \rangle_{w} \)
   - No parameters in this step!
*** Notice!                                                    :B_alertblock:
	:PROPERTIES:
	:BEAMER_env: alertblock
	:END:
  - \( \sigma \) is the "overall" step size and is a scalar.
  - It could also be a matrix. Is this a good idea?
	- What about a diagonal matrix?
*** Computing?
   - Use elementwise addition using ~+~ operator
***                                                                  :B_note:
	:PROPERTIES:
	:BEAMER_env: note
	:END:
	- Having \( \sigma \) as matrix is not a good idea because one dimension
      depends on another---a which complicates stuff for a black box algorithm.
	- Besides that's precisely what the \( \bv{C} \) encodes---both rotation and
      scaling.
	- So choose only a scalar.
** Third step : Recombination--Answers
  - \(\sigma\) could also be a matrix. Is this a good idea?
	- *NO*!
	- One dimension depends on another, but not during sampling. This degrades
      the convergence of the algorithm
  - What about a diagonal matrix?
	- *NO*!
	- \bv{C}= \bv{BD^2B^T} does the job of maintaining scaling, orientation etc. of the elements.

  *CONCLUSION*---Scalar \(\sigma\) is apt.
** Fourth step : Step size control
*** Control for \( \sigma \) and cumulation \(\gv{p}_{\sigma} \)
   - \( \gv{p}_\sigma \leftarrow (1 - c_\sigma) \gv{p}_\sigma +
     \sqrt{c_\sigma \left( 2 - c_\sigma \right)} \mu_{\text{cov} }
     \bv{C}^{-\frac{1}{2}} \langle \gv{z} \rangle_{w} \)
   - \( \sigma \leftarrow \sigma \exp{\left( \frac{c_\sigma}{d_\sigma} \left[
     \tfrac{ \norm{\gv{p}_\sigma} }{ \mathsf{E} \norm{ \mathcal{N}\left( {0},
     \bv{I} \right) } } - 1 \right] \right)} \)

*** Computing/Python?
   + Notice you need to invert the covariance matrix! How will you do it?
	 + *Hint*: Exploit properties of \bv{C}!
	 + This means you just need ~np.linalg.eigh()~ for now (there are many other
       powerful methods for general symmetric matrix inverse)
	 + Can reduce \( \order{n}^3\) to \(\order{n}^2 \) in practice? ( See B2.
       Strategy internal numerical effort in CMA tutorial)

***                                                                  :B_note:
	:PROPERTIES:
	:BEAMER_env: note
	:END:
	- Positive definiteness is the property. Show a demo of how positive
      definiteness used to invert.
	- Spectral theorem : Symmetric matrices have a complete set of eigenvectors
      (no generalized EV needed)
	- PD : All positive eigenvalues needed
** Step size control: Computing/Python
*** Computing continued
   + Extensive use of matvecs (~@~)
   + What about the norm in the \( \sigma \) update?
	 + What is a norm?
	 + So what norm should we use?
	   + The two-norm is widely used
   + What's \( \mathsf{E} \)?
	 - What's \( \mathsf{E} \norm{ \mathcal{N}\left( {0}, \bv{I} \right) } \)?
	   - \( \approx \sqrt{n} \left( 1 - \tfrac{1}{4n} + \tfrac{1}{21n^2} \right) \)
***                                                                  :B_note:
	:PROPERTIES:
	:BEAMER_env: note
	:END:
	- Expected length of distribution
** Step size control \rightarrow math
*** What is path update doing?
	- Increase probability of reproducing successful solution paths...
	- Weighting with exponential decay...
	- What about the choice of weights?
	  - Makes the expected length independent of the direction
	  - "Follows" the random choice of \( \gv{p}^{(0)}_\sigma\)
*** What is \(\sigma\) update doing?
	- Decrease/Increase size until path steps are uncorrelated...
	- How does the two norm of the path reflect this "un"correlation?
	- What about the choice of weights?
***                                                                  :B_note:
	:PROPERTIES:
	:BEAMER_env: note
	:END:
	- Two norm weighted by expectation tells you how much deviation is there in
      the expectation...
** Step size : Parameters
*** Choice of \( c_\sigma , d_\sigma \)?
	- Look at the CMA tutorial : [[file:~/Desktop/Masters_Resources/readings/optimization/Hansen/The%20CMA%20evolution%20strategy%20A%20tutorial.pdf][The CMA tutorial]]/[[https://arxiv.org/pdf/1604.00772.pdf][CMA tutorial on Arxiv]]
	- \( c_\sigma \) is learning rate for cumulation usually set to \( \approx
      \tfrac{4}{n} \)
	- \( d_\sigma \) is the damping parameter for step size update \(\approx 1 + \frac{\mu_{\text{cov}}}{\sqrt{n}} \)
	# \[ \scalebox{1.5}{$\tfrac{\mu_{ \text{cov} } + 2 }{ n + 5 + \mu_{ \text{cov} }
	# }$} \]
** Fifth step : Covariance matrix adaptation
*** Control for \( \bv{C} \) and cumulation \(\gv{p}_{c} \)
   - \( \gv{p}_c \leftarrow (1 - c_c) \gv{p}_c+
     \sqrt{c_c\left( 2 - c_c\right) } \mu_{\text{cov}}
     \langle \gv{z} \rangle_{w} \)
   - \( \bv{C} \leftarrow (1 - c_{\text{cov}}) \bv{C} +
     \frac{c_{\text{cov}}}{\mu_{\text{cov}}} \gv{p}_{c} \gv{p}^T_c +
     c_{\text{cov}} \left( 1 - \frac{1}{\mu_{cov}}\right) \bv{Z}  \)
	 where \( \bv{Z} =  \sum_{i=1}^{\mu} w_i \gv{z}_{i:\lambda} \gv{z}^T_{i:\lambda}\)
*** Computing/Python?
   + Usual operations (~*,+~)
   + For calculating outer products, use ~np.outer()~

** CMA \rightarrow math
*** What is cumulation for \(\gv{p}_c\) doing?
	- Weighting with exponential decay for prior values
	- New information from PCA of steps updated into \( \bv{C} \) path
	- What about the choice of weights?
*** What is \(\bv{C}\) update doing?
	- Weighting with exponential decay for prior values
	- Rank one update using \( \gv{p}_c \) (What's *rank*?)
	  - Why is the update rank one? (One-dimensional information)
	  - Why use \( \gv{p}_c \) rather than \( \langle z \rangle\)?
	- Rank \( \mu \) update
	  - As seen earlier, CMA cleverly estimates \( \bv{C} \) using old step
        information
***                                                                  :B_note:
	:PROPERTIES:
	:BEAMER_env: note
	:END:
	- Rank demo using \( [1 2 3] \)
	- using \( \langle z \rangle\) loses information about correlation between
      steps, the history informatino (Explain that this may lead to effects on
      path length adaption and so on)

** CMA: Parameters
*** Choice of \( c_c, c_{\text{cov}}\)?
	- Look at the CMA tutorial : [[file:~/Desktop/Masters_Resources/readings/optimization/Hansen/The%20CMA%20evolution%20strategy%20A%20tutorial.pdf][The CMA tutorial]]/[[https://arxiv.org/pdf/1604.00772.pdf][CMA tutorial on Arxiv]]
	- \( c_c\) is learning rate for path cumulation set to \( \approx
      \tfrac{4}{n} \)
	- \( c_{\text{cov}} \approx \tfrac{2 + \mu^2_{\text{cov}}}{n^2} \)

** Terminating CMA
*** Algorithm should be stopped when CPU-time is wasted. Then we can:
	  1) restart (eventually with increased population size)
	  2) reconsider encoding and objective function formulation
*** Problem independent
	- ~NoEffectAxis~ : Stop if adding \(0.1\) std.dev. vector to any direction
       of basis \( \bv{B} \) does not change \( \gv{m} \)
	- ~NoEffectCoord~ : Stop if adding \(0.2\) std.dev. to any coordinate does not change \( \gv{m} \)
	- ~ConditionCov~: stop if condition number of covariance matrix exceeds \(
      10^{14} \)
	  - Whats condition number of a matrix?
	  - ~np.linalg.cond()~, although you can directly check ~D~
***                                                                  :B_note:
	:PROPERTIES:
	:BEAMER_env: note
	:END:
	- First criteria is explanatory: when the c matrix is small, it will still
      choose yours as an optima
	- ~NoEffectCoord~ : \( m_i = m_i + 0.2 \sigma c_i \)
	- using \( \langle z \rangle\) loses information about correlation between
      steps, the history informatino (Explain that this may lead to effects on
      path length adaption and so on)
	- Condition number tells you stretch of matrix. If 10^14 you can go home.

** Terminating CMA contd.
*** Problem independent
  	- ~EqualFunValues~: stop if the range of the best \( \gv{f}(\gv{x}) \) of
      the last \( 10 + \lceil 30n/\lambda \rceil \) generations is zero.
	- ~Stagnation~: Track history of the best and the median fitness in each
      iteration over the last \( 20 \%\) but at least \( 120+30n/\lambda \) and
      no more than \( 20000\) iterations. Stop, if in both histories the median
      of the last (most recent) \( 30 \% \) values is not better than the median
      of the first \( 30\%\).
	- ~TolXUp~: stop if \( \sigma \times max(diag(\bv{D})) \) increased by more
      than \( 10^4\). This indicates a far too small initial \( \sigma \), or
      divergence.
***                                                         :B_ignoreheading:
	:PROPERTIES:
	:BEAMER_env: ignoreheading
	:END:
	*We note that there are problem dependent diagnostics too!*
***                                                                  :B_note:
	:PROPERTIES:
	:BEAMER_env: note
	:END:
	- Equalfunvalus is self explanatory
	- Average properties of the simualtion does not improve
	- Toelrance limit reached

** Boundaries/Constraints in CMA : Best solution strictly inside
	- Set fitness (for minimization problem) as
\[ f_{\text{fitness}} (\gv{x}) = f_{\text{max}} + \norm{\gv{x} -
\gv{x}_{\text{feasible}}} \]
	    1. Notation
		   1) \( f_{\text{max}} \) is larger than worst feasible fitness
		   2) \( \gv{x}_{\text{feasible}} \) is constant,in the middle of feasible region
	    2. Caveat : Optimal solution not too close to the infeasible region


	- Alternatively, resample any infeasible point until it becomes feasible

** Boundaries/Constraints in CMA : Repair
	- Simply "repair" infeasible individuals (say when boundary is a box) before
      update so that they satisfy the constraint
	  1. Caveat : Repairs are dangerous
		 - Distribution affected by repair, hurting CMA's convergence
	  2. "Re-repair" mechanisms to prevent divergence are also reported

	- Alternatively, penalize the repaired solutions
	\[  f_{\text{fitness}} (\gv{x}) = f(\gv{x}_{\text{repaired}}) + \alpha \norm{\gv{x} -
\gv{x}_{\text{repaired}}}^2 \]

* Comparing CMA against GA
** CMAes vs GA--setup
*** Optimization on smooth functions
	- Two dimensional, \( C^{\infty} \) functions \( f(\gv{x}) \) : \( \left(\mathbb{R}^2,  \mathbb{R}, f, \leq \right) \)
	- shifted Schaffer function (optima in the middle well)
***                                                         :B_ignoreheading:
	:PROPERTIES:
	:BEAMER_env: ignoreheading
	:END:
	 [[file:images/schaffer.png]]

** CMAes vs GA--setup
***                                                         :B_ignoreheading:
	:PROPERTIES:
	:BEAMER_env: ignoreheading
	:END:
	- shifted Rastrigin function (optima in the middle well)
	[[file:images/rastrigin.png]]

** Comparison between functions[fn:2]
***                                                                :B_column:
	:PROPERTIES:
	:BEAMER_env: column
	:BEAMER_col: 0.5
	:END:
#+CAPTION: Schaffer--setup
[[file:images/schaffer_start.png]]
***                                                                :B_column:
	:PROPERTIES:
	:BEAMER_env: column
	:BEAMER_col: 0.5
	:END:
#+CAPTION: Rastrigin--setup
[[file:images/rastrigin_start.png]]

*** Lighter region indicates smaller values                 :B_ignoreheading:
	:PROPERTIES:
	:BEAMER_env: ignoreheading
	:END:
	Lighter region indicates smaller values
** Simple ES
*** Scheme
	- *Sampling* : \( \gv{z}_i \sim \mathcal{N}\left( \gv{m}, \bv{C} \right) \)
	- *Mean-update* : \( \gv{m} \leftarrow \gv{z}_{1:\lambda} \)
	- *Covariance-update* : \( \bv{C} = \begin{bmatrix}\sigma^2_x & \sigma_x \sigma_y \\  \sigma_x
      \sigma_y & \sigma^2_y \end{bmatrix} \) \( \sigma_x, \sigma_y \) are fixed.
	- No other updates (on path etc.)
*** Legend
	- @@latex:{\color{shamrockgreen}@@Green@@latex:}@@ : Tracks the mean \(\gv{m}\).
	- @@latex:{\color{royalblue}@@Blue@@latex:}@@ : Tracks the sampled solutions
      at generetation \(g\).
	- @@latex:{\color{scarlet}@@Red@@latex:}@@ : Tracks the best individual so
      far.
*** Results
	Simple Evolution strategy from [[http://blog.otoro.net/2017/10/29/visual-evolution-strategies/][Otoro]] shown for 20 generations
** Simple ES-Observations
*** Convergence
	- What do you expect for general problems?
	-
*** Rate of convergence
	- Is this fast/slow convergence?
	-
*** Number of function evaluations?
	- High? Low? Not bad?
	-

** Simple ES-Observations
*** Convergence
	- What do you expect for general problems?
	- *Will get stuck--lack of diversity, keeps only best population* (See
      rastrigin, which temporarily gets stuck)
	- *Heavy* parameter dependence too
*** Rate of convergence
	- Is this fast/slow convergence?
	- *Slow--no history information*
*** Number of function evaluations?
	- High? Low? Not bad?
	- *Decent--but no promises for real life black-box optimization problems*

** Simple GA
*** Scheme
	- *Environmental selection* : Keep only best \( 10 \% \)
	- *Sampling* : Crossover from parents selected above with \( p_c = 1 \)
	- *Crossover* : Select two parents, obtain \(x\) or \(y\) from either parent
      with \( 0.5 \) probability (two coin tosses)
	- *Mutation* : Introduce Gaussian noise with fixed \( \sigma \)
	- No other updates (on path etc.)
*** Legend
	- @@latex:{\color{shamrockgreen}@@Green@@latex:}@@ : Tracks the elites from
      prior generation \(g\).
	- @@latex:{\color{royalblue}@@Blue@@latex:}@@ : Offsprings from candidate solutions.
	- @@latex:{\color{scarlet}@@Red@@latex:}@@ : Tracks the best individual so
      far.

** Simple GA-Observations
*** Convergence
	- What do you expect for general problems?
	-
*** Rate of convergence
	- Is this fast/slow convergence?
	-
*** Number of function evaluations?
	- High? Low? Not bad?
	-

** Simple GA-Observations
*** Convergence
	- What do you expect for general problems?
	- *Will get stuck--lack of diversity, keeps only elitist population*
	- *Heavy* parameter dependence
	- *Tracks* modality well (for both Schaffer and Rastrigin)
*** Rate of convergence
	- Is this fast/slow convergence?
	- *Slower* than simple ES
*** Number of function evaluations?
	- High? Low? Not bad?
	- *High*

** CMAes-Observations
*** Can you spot the updates?
	- \( \gv{m} \) update (fairly obvious)
	- Step size update
	  - Path update
	- Covariance matrix upfate
	  - Rank \( \mu \) updates
	  - Rank \( 1\) update (Path update)
***                                                                  :B_note:
	:PROPERTIES:
	:BEAMER_env: note
	:END:
	- Step size : Initially small, As soon as it sees all are moving in one
      direction, it quickly adapts in both cases.
	- Step size reduce. Rastrigin more difficult as
      optima close to zero, so step size reduce takes time.
	- Cov update : Direction is clearly important. First recognizes the ascent
      direction as a prinicpal component. Then of course the next one is
      complementary to it...
	- Most due to rank \( \mu \) update (as scaling applied there)
	- But rank one update also seen...

** CMAes-Observations
*** Convergence
	- What do you expect for general problems?
	- *Good* for problems of "moderate" dimensions
*** Rate of convergence
	- Is this fast/slow convergence?
	- *Fast* (Approximately brackets minima in \( \order{n} \) functional evaluations)
*** Number of function evaluations?
	- High? Low? Not bad?
	- Low (same as above)
** CMAes-Some interesting videos
	  - Mario https://www.youtube.com/watch?v=0iipyd7Gi70
	  - Rastrigin : https://www.youtube.com/watch?v=aP31Q7o2UGU
	  - Biped: https://www.youtube.com/watch?v=lOaWvOA9cb4
	  - Robot Invivo: https://www.youtube.com/watch?v=trR2Gc1tLzg
	  - Robot invitro: https://www.youtube.com/watch?v=fjTd06L-9bQ
	  - Knifwfisj : https://www.youtube.com/watch?v=3XjgZbs0t2g
	  - https://blog.openai.com/evolution-strategies/

* Footnotes

[fn:4] [[https://www.slideshare.net/OsamaSalaheldin2/cmaes-presentation][CMAes overview, Slideshare]]

[fn:2] [[http://blog.otoro.net/2017/10/29/visual-evolution-strategies/][Otoro]]

[fn:1] https://math.stackexchange.com/q/2115701
