#+TITLE: Covariance Matrix Adaptation in Python
#+AUTHOR: /Tejaswin Parthasarathy/, Mattia Gazzola
#+SUBTITLE: ME498: Comp. modeling & optimization
#+BEAMER_FRAME_LEVEL: 2
# #+BEAMER_HEADER: \institute[INST]{Institute\\\url{http://www.institute.edu}}
# #+BEAMER_HEADER: \titlegraphic{\includegraphics[height=1.5cm]{test}}

#+STARTUP: beamer
#+LATEX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [presentation]
# #+LATEX_CLASS_OPTIONS: [notes]
#+LATEX_HEADER:\usetheme[progressbar=frametitle]{metropolis}
#+LATEX_HEADER:\usepackage{tikz}
#+LATEX_HEADER:\usetikzlibrary{backgrounds,matrix,fit,calc}
#+LATEX_HEADER:\usepackage{pgfplots}
#+LATEX_HEADER:\pgfplotsset{compat=1.16}
#+LATEX_HEADER:\usepackage{nicematrix}
#+LATEX_HEADER:\usepackage{spot}
#+LATEX_HEADER:\usepackage[beamer,customcolors]{hf-tikz}
#+LATEX_HEADER:\newcommand{\gv}[1]{\ensuremath{\mbox{\boldmath$ #1 $}}}
#+LATEX_HEADER:\newcommand{\bv}[1]{\ensuremath{\mathbf{#1}}}
#+LATEX_HEADER:\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
#+LATEX_HEADER:\newcommand{\order}[1]{\mathcal O \left( #1 \right)} % order of magnitude
#+LATEX_HEADER:\newcommand*{\Scale}[2][4]{\scalebox{#1}{$#2$}}%
#+LATEX_HEADER:\definecolor{scarlet}{rgb}{1.0, 0.13, 0.0}
#+LATEX_HEADER:\definecolor{shamrockgreen}{rgb}{0.0, 0.62, 0.38}
#+LATEX_HEADER:\definecolor{royalblue}{rgb}{0.25, 0.41, 0.88}
#+LATEX_HEADER:\definecolor{metropolisorange}{RGB}{235,129,27}
#+LATEX_HEADER:\definecolor{deeppink}{RGB}{205,16,118}
#+LATEX_HEADER:\definecolor{burple}{RGB}{104,50,227}
#+LATEX_HEADER: \setmonofont{Iosevka Semibold}
#+OPTIONS:   H:2 num:t toc:nil ::t |:t ^:{} -:t f:t *:t <:t
#+OPTIONS:   tex:t d:nil todo:t pri:nil tags:nil
#+COLUMNS: %45ITEM %10BEAMER_ENV(Env) %10BEAMER_ACT(Act) %4BEAMER_COL(Col) %8BEAMER_OPT(Opt)

# LATEXX EXPORTS AT START
#+begin_export latex
	\pgfplotsset{
	colormap={whitered}{color(0cm)=(white); rgb255(1cm)=(235,129,27)}
	}
#+end_export
* ~Matplotlib~
** Additional packages
  - ~Seaborn~  *DEMO*
  - ~bokeh~
  - ~plotly~
  - See [[https://wiki.python.org/moin/NumericAndScientific/Plotting][Python wiki]] for more plotting tools
** Quick review
*** GA demo : Results of our GA implementation
*** How to improve convergence?
	+ Parameters?
	+ Strategies? (More mutation, less recombination say...?)
	+ Tuning is painful
*** \Rightarrow CMAes (and other algorithms)
* Implementation of CMAes
** The algorithm
# #+LATEX: \footnotesize
# #+CAPTION: CMAes
# #+ATTR_LATEX: :width 1.03\textwidth
# [[file:images/cma_algo.001.jpeg]]
 Set \( \mathbf{m} = \mathbf{0}, \mathbf{C} = \mathbf{I}, \sigma =
 0.5, \mathbf{p}_c = \mathbf{0}, \mathbf{p}_{\sigma} = \mathbf{0}\)
*** Sampling                                                        :B_block:
	:PROPERTIES:
	:BEAMER_env: block
	:END:

	  \[ \begin{aligned}
	  \tikzmarkin<1>[set fill color=shamrockgreen!20, set border color=black]{sampling}(0.2,-0.4)(-0.2,0.6)
	  \mathbf{z}_{i} & \sim \mathcal{N}(\mathbf{0}, \mathbf{C}) \\
	  \mathbf{x}_{i} &= m+\sigma \mathbf{z}_{i} \tikzmarkend{sampling}
	  \end{aligned} \]

*** Selection and Recombination                                     :B_block:
	:PROPERTIES:
	:BEAMER_env: block
	:END:

	\( \mu \) indicates best individuals
	  \[ \begin{aligned}
	  \tikzmarkin<1>[set fill color=royalblue!40, set border color=black]{selec}(0.2,-0.4)(-0.2,0.8)
	  \langle\mathbf{z}\rangle_{w} &= \sum_{i=1}^{\mu} w_{i} \mathbf{z}_{i : \lambda} \\
	  \mathbf{m} &\longleftarrow \mathbf{m}+\sigma\langle\mathbf{z}\rangle_{w} \tikzmarkend{selec}
	  \end{aligned} \]

** The algorithm contd.
*** Step size update                                                :B_block:
	:PROPERTIES:
	:BEAMER_env: block
	:END:
	 \[ \begin{aligned}
	 \tikzmarkin<1>[set fill color=scarlet!30, set border color=black]{step}(5.2,-0.4)(-0.2,0.8)
	 \mathbf{p}_{\sigma} &\longleftarrow\left(1-c_{\sigma}\right)
	 \mathbf{p}_{\sigma}+\sqrt{1-\left(1-c_{\sigma}\right)^{2}}
	 \sqrt{\frac{1}{\sum_{i=1}^{\mu} w_{i}^{2}}}
	 \mathbf{C}^{-\frac{1}{2}}\langle\mathbf{z}\rangle_{w} \\
	 \sigma &\longleftarrow \sigma
	 e^{\left(\frac{1}{d_{\sigma}}\left(\frac{\left\|p_{\sigma}\right\|}{E\|\mathcal{N}(\mathbf{0},
	 \mathbf{I})\|}-1\right)\right)} \tikzmarkend{step} \\
	 \end{aligned} \]

** The algorithm contd..
*** Covariance Matrix Update                                        :B_block:
	:PROPERTIES:
	:BEAMER_env: block
	:END:
	   \[ \begin{aligned}
	   \tikzmarkin<1>[set fill color=metropolisorange!40, set border color=black]{cov}(0.4,-0.8)(-0.4,0.8)
	   \mathbf{p}_{c} &\longleftarrow \left(1-c_{c}\right)
	   \mathbf{p}_{c}+\sqrt{1-\left(1-c_{c}\right)^{2}} \sqrt{\frac{1}{\sum_{i=1}^{\mu}
	   w_{i}^{2}}}\langle\mathbf{z}\rangle_{w} \\
	   \mathbf{Z} &= \sum_{i=1}^{\mu} w_{i} \mathbf{z}_{i : \lambda} \mathbf{z}_{i :
	   \lambda}^{T} \\
	   \mu_{c o v}&=\sqrt{\frac{1}{\sum_{i=1}^{\mu} w_{i}^{2}}} \\
	   \mathbf{C} &\longleftarrow\left(1-c_{c o v}\right) \mathbf{C}+c_{c o v}
	   \frac{1}{\mu_{c o v}} \mathbf{p}_{c} \mathbf{p}_{c}^{T}+c_{c o
	   v}\left(1-\frac{1}{\mu_{c o v}}\right) \mathbf{Z} \tikzmarkend{cov}
	   \end{aligned} \]
** Starting CMAes
*** Problem independent
   - Set evolution paths \( \gv{p}_\sigma = \gv{0}, \gv{p}_c = \gv{0}\)
   - Set number of generations \( g = 0 \)
   - Set covariance matrix \( \bv{C} = \bv{I} \) (Why?)
*** Problem dependent
   - Distribution mean \( \gv{m} \in \mathbb{R}^n \)
   - Step size \( \sigma \in \mathbb{R}_{>0} \) (Important to set \( >0\) )
** Starting CMAes: more on problem dependent parameters
   - Optimum presumably be within the initial cube \( \gv{m} \pm 3 \sigma
     \left(1 ,1 , \cdots, 1 \right)^T\)
   - \( \therefore \) if optima \( \in [a, b]^{n} \) choose \( \gv{m} \in [a,b]^{n}
     \) (as a uniformly random vector) and \( \sigma = 0.3*(b-a) \)
   - Different search intervals \( \Delta s_i \) for different variables can be
     done using \( \bv{C} \) as shown below (deferred discussion):

   #+NAME: lyap_asym
   \begin{equation}
   \begin{aligned}
   \begin{bmatrix}
   \Delta s^2_1 & 0 & \cdots & 0 \\
   0 & \Delta s^2_2 & \cdots & 0 \\
   \vdots & \ddots & \ddots & \vdots \\
   0 & 0 &  \cdots & \Delta s^n_2  \\
   \end{bmatrix}
   \end{aligned}
   \end{equation}

   - \( \Delta s_i \) all must be of similar magnitude (for conditioning). Else,
     rescale your problem.
** First step : Sampling
*** New population of points, for \( k = 1 \cdots \lambda \)
   - \( \gv{y}_k \sim \mathcal{N}\left( \gv{0}, \bv{I} \right) \)
   - \( \gv{z}_k \sim \mathcal{N}\left( \gv{0}, \bv{C} \right) =
     \bv{B}\bv{D}\gv{y}_k \)
	 - Given \( \bv{C} = \bv{B}\bv{D}^2\bv{B}^T \)
	 - Consult [fn:1]  for a proof of why \( \bv{A} \mathcal{N}\left(\gv{0}, \bv{I}
       \right) = \mathcal{N} \left(\gv{0}, \bv{A}\bv{A}^T \right) \)
   - \( \gv{x}_k = \gv{m} + \sigma \gv{z}_k \sim \mathcal{N}\left( \gv{m},
     \sigma^2 \bv{C} \right) \)
*** Computing?
   - Steps 1 and 3 above using ~*/np.multiply~ for
     elementwise multiplication and ~+~ for elementwise addition
*** We need a way to sample correlated (across dimensions) populations from ~numpy~ :B_ignoreheading:
	:PROPERTIES:
	:BEAMER_env: ignoreheading
	:END:
   *We need a way to sample correlated (across dimensions) populations from* ~numpy~
** Sampling : Python
*** How to sample a multivariate normal distribution?
	- ~np.random.multivariate_normal~ *DEMO*
*** Caveats?
	- What is \( \bv{C}\) / ~cov~ (in a 2D case) and its meaning?
	- ~cov~ needs to be SP(semi)D. Is it? What about the update step?
	- What happens in ~numpy~ if it is not?
***                                                                  :B_note:
	:PROPERTIES:
	:BEAMER_env: note
	:END:
	- \( \mu_x = \frac{1}{N} \sum_{i=1}^{N}x_i \)
	- \( \sigma_x^2 = \frac{1}{N} \sum_{i=1}^{N}(x_i - \mu_x)^2 \) and \(
      \sigma_{xy} = \frac{1}{N} \sum_{i=1}^{N}(x_i - \mu_x)(y_i - \mu_y) \)
	- Show symmetric nature of ~cov~ after update
	- Show positive definiteness (use PD of C, as well as the fact that it is
	  made up of rank-one decompositions
	- ~numpy~ checks for PD and throws an error
	- Now explain why you set \( A=CC^T \)
** Normal Distribution[fn:3]
#+CAPTION: The Normal Distribution
[[file:images/The_Normal_Distribution.pdf]]
** TODO Covariance Matrix [fn:4]
*** \( \mathcal{N}(\gv{0}, \bv{I})\)                                :B_block:
	:PROPERTIES:
	:BEAMER_env: block
	:BEAMER_COL: 0.5
	:END:
	#+begin_export latex
	\begin{center}
		\begin{tikzpicture}[
			declare function={mu1=0;},
			declare function={mu2=0;},
			declare function={sigma1=1;},
			declare function={sigma2=1;},
			declare function={normal(\m,\s)=1/(2*\s*sqrt(pi))*exp(-(x-\m)^2/(2*\s^2));},
			declare function={bivar(\ma,\sa,\mb,\sb)=
				1/(2*pi*\sa*\sb) * exp(-((x-\ma)^2/\sa^2 + (y-\mb)^2/\sb^2))/2;}, scale=0.6]
			\begin{axis}[
				colormap name=whitered,
				view={45}{65},
				enlargelimits=false,
				grid=major,
				domain=-2.5:2.5,
				y domain=-2.5:2.5,
				samples=26,
				xlabel=$x_1$,
				ylabel=$x_2$,
				zlabel={$\mathcal{N}$},
				colorbar,
				% colorbar style={
				% 	at={(1,0)},
				% 	anchor=south west,
				% 	height=0.25*\pgfkeysvalueof{/pgfplots/parent axis height},
				% 	title={$P(x_1,x_2)$}
				% }
			]
			\addplot3 [surf] {bivar(mu1,sigma1,mu2,sigma2)};
			\addplot3 [domain=-2.5:2.5,samples=31, samples y=0, thick, smooth] (x,2.5,{normal(mu1,sigma1)});
			\addplot3 [domain=-2.5:2.5,samples=31, samples y=0, thick, smooth] (-2.5,x,{normal(mu2,sigma2)});

			\draw [black!50] (axis cs:-2.5,0,0) -- (axis cs:2.5,0,0);
			\draw [black!50] (axis cs:0,-2.5,0) -- (axis cs:0,2.5,0);

			% \node at (axis cs:-1,1,0.18) [pin=165:$P(x_1)$] {};
			% \node at (axis cs:1.5,4,0.32) [pin=-15:$P(x_2)$] {};
			\end{axis}
		\end{tikzpicture}
	\end{center}
	#+end_export

*** \( \mathcal{N}(\gv{\mu}, \bv{I})\)                              :B_block:
	:PROPERTIES:
	:BEAMER_env: block
	:BEAMER_COL: 0.5
	:END:

	#+begin_export latex
	\begin{center}
		\begin{tikzpicture}[
			declare function={mu1=1;},
			declare function={mu2=-0.5;},
			declare function={sigma1=1;},
			declare function={sigma2=1;},
			declare function={normal(\m,\s)=1/(2*\s*sqrt(pi))*exp(-(x-\m)^2/(2*\s^2));},
			declare function={bivar(\ma,\sa,\mb,\sb)=
				1/(2*pi*\sa*\sb) * exp(-((x-\ma)^2/\sa^2 + (y-\mb)^2/\sb^2))/2;}, scale=0.6]
			\begin{axis}[
				colormap name=whitered,
				view={45}{65},
				enlargelimits=false,
				grid=major,
				domain=-2.5:2.5,
				y domain=-2.5:2.5,
				samples=26,
				xlabel=$x_1$,
				ylabel=$x_2$,
				zlabel={$\mathcal{N}$},
				colorbar,
			]
			\addplot3 [surf] {bivar(mu1,sigma1,mu2,sigma2)};
			\addplot3 [domain=-2.5:2.5,samples=31, samples y=0, thick, smooth] (x,2.5,{normal(mu1,sigma1)});
			\addplot3 [domain=-2.5:2.5,samples=31, samples y=0, thick, smooth] (-2.5,x,{normal(mu2,sigma2)});

			\draw [black!50] (axis cs:-2.5,0,0) -- (axis cs:2.5,0,0);
			\draw [black!50] (axis cs:0,-2.5,0) -- (axis cs:0,2.5,0);

			\end{axis}
		\end{tikzpicture}
	\end{center}
	#+end_export
***                                                                  :B_note:
	:PROPERTIES:
	:BEAMER_env: note
	:END:
	- Independence and identical distributions
	- Intution from Brad Osgood's notes on Fourier Transforms 3.7
	- End with CMA discovers the dependence and distribution of each random variable
** Covariance Matrix
*** \( \mathcal{N}(\gv{0}, \bv{I})\)                                :B_block:
	:PROPERTIES:
	:BEAMER_env: block
	:BEAMER_COL: 0.5
	:END:
	#+begin_export latex
	\begin{center}
		\begin{tikzpicture}[
			declare function={mu1=0;},
			declare function={mu2=0;},
			declare function={sigma1=1;},
			declare function={sigma2=1;},
			declare function={normal(\m,\s)=1/(2*\s*sqrt(pi))*exp(-(x-\m)^2/(2*\s^2));},
			declare function={bivar(\ma,\sa,\mb,\sb)=
				1/(2*pi*\sa*\sb) * exp(-((x-\ma)^2/\sa^2 + (y-\mb)^2/\sb^2))/2;}, scale=0.6]
			\begin{axis}[
				colormap name=whitered,
				view={45}{65},
				enlargelimits=false,
				grid=major,
				domain=-2.5:2.5,
				y domain=-2.5:2.5,
				samples=26,
				xlabel=$x_1$,
				ylabel=$x_2$,
				zlabel={$\mathcal{N}$},
				colorbar,
			]
			\addplot3 [surf] {bivar(mu1,sigma1,mu2,sigma2)};
			\addplot3 [domain=-2.5:2.5,samples=31, samples y=0, thick, smooth] (x,2.5,{normal(mu1,sigma1)});
			\addplot3 [domain=-2.5:2.5,samples=31, samples y=0, thick, smooth] (-2.5,x,{normal(mu2,sigma2)});

			\draw [black!50] (axis cs:-2.5,0,0) -- (axis cs:2.5,0,0);
			\draw [black!50] (axis cs:0,-2.5,0) -- (axis cs:0,2.5,0);
			\end{axis}
		\end{tikzpicture}
	\end{center}
	#+end_export

*** \( \mathcal{N}(\gv{0}, \bv{C})\)                                :B_block:
	:PROPERTIES:
	:BEAMER_env: block
	:BEAMER_COL: 0.5
	:END:

	#+begin_export latex
	\begin{center}
		\begin{tikzpicture}[
			declare function={mu1=0;},
			declare function={mu2=0;},
			declare function={sigma1=0.5;},
			declare function={sigma2=2;},
			declare function={normal(\m,\s)=1/(2*\s*sqrt(pi))*exp(-(x-\m)^2/(2*\s^2));},
			declare function={bivar(\ma,\sa,\mb,\sb)=
				1/(2*pi*\sa*\sb) * exp(-(((x*cos(60) + y*sin(60))-\ma)^2/\sa^2 + ((-x*sin(60) + y*cos(60))-\mb)^2/\sb^2))/2;}, scale=0.6]
			\begin{axis}[
				colormap name=whitered,
				view={45}{65},
				enlargelimits=false,
				grid=major,
				domain=-2.5:2.5,
				y domain=-2.5:2.5,
				samples=26,
				xlabel=$x_1$,
				ylabel=$x_2$,
				zlabel={$\mathcal{N}$},
				colorbar,
			]
			\addplot3 [surf] {bivar(mu1,sigma1,mu2,sigma2)};
			% \addplot3 [domain=-2.5:2.5,samples=31, samples y=0, thick, smooth] (x,2.5,{normal(mu1,sigma1)});
			% \addplot3 [domain=-2.5:2.5,samples=31, samples y=0, thick, smooth] (-2.5,x,{normal(mu2,sigma2)});

			\draw [black!50] (axis cs:-2.5,0,0) -- (axis cs:2.5,0,0);
			\draw [black!50] (axis cs:0,-2.5,0) -- (axis cs:0,2.5,0);

			\end{axis}
		\end{tikzpicture}
	\end{center}
	#+end_export
** Covariance Matrix : Example 1
  \[ \bv{C} = \begin{bmatrix} 1 & 0 \\ 0 & 1\end{bmatrix}\]
*** \( \mathcal{N}(\gv{0}, \bv{I})\)                               :B_column:
	:PROPERTIES:
	:BEAMER_env: column
	:BEAMER_COL: 0.5
	:END:
	#+begin_export latex
	\begin{center}
		\begin{tikzpicture}[
			declare function={mu1=0;},
			declare function={mu2=0;},
			declare function={sigma1=1;},
			declare function={sigma2=1;},
			declare function={normal(\m,\s)=1/(2*\s*sqrt(pi))*exp(-(x-\m)^2/(2*\s^2));},
			declare function={bivar(\ma,\sa,\mb,\sb)=
				1/(2*pi*\sa*\sb) * exp(-((x-\ma)^2/\sa^2 + (y-\mb)^2/\sb^2))/2;}, scale=0.6, baseline]
			\begin{axis}[
				colormap name=whitered,
				view={45}{65},
				enlargelimits=false,
				grid=major,
				domain=-2.5:2.5,
				y domain=-2.5:2.5,
				samples=26,
				xlabel=$x_1$,
				ylabel=$x_2$,
				zlabel={$\mathcal{N}$},
				colorbar,
			]
			\addplot3 [surf] {bivar(mu1,sigma1,mu2,sigma2)};
			\addplot3 [domain=-2.5:2.5,samples=31, samples y=0, thick, smooth] (x,2.5,{normal(mu1,sigma1)});
			\addplot3 [domain=-2.5:2.5,samples=31, samples y=0, thick, smooth] (-2.5,x,{normal(mu2,sigma2)});

			\draw [black!50] (axis cs:-2.5,0,0) -- (axis cs:2.5,0,0);
			\draw [black!50] (axis cs:0,-2.5,0) -- (axis cs:0,2.5,0);
			\end{axis}
		\end{tikzpicture}
	\end{center}
	#+end_export
	#+begin_src python :exports none
	  import numpy as np
	  import os

	  DATA_PATH = './data/'
	  if not os.path.isdir(DATA_PATH):
		  os.makedirs(DATA_PATH)

	  # Generate data for all cases

	  # First case, normal distribution centered at origin
	  mean = np.zeros((2,))
	  cov = np.eye(2)
	  sampled_dist = np.random.multivariate_normal(mean, cov, 100)
	  np.savetxt(os.path.join(DATA_PATH, 'normal.txt'), sampled_dist, delimiter='\t')

	  # Second case, skewed distribution in X
	  mean = np.zeros((2,))
	  cov = np.eye(2)
	  cov[1,1] = 0.2
	  sampled_dist = np.random.multivariate_normal(mean, cov, 100)
	  np.savetxt(os.path.join(DATA_PATH, 'skewedX.txt'), sampled_dist, delimiter='\t')

	  # Third case, skewed distribution in Y
	  mean = np.zeros((2,))
	  cov = np.eye(2)
	  cov[0,0] = 0.2
	  sampled_dist = np.random.multivariate_normal(mean, cov, 100)
	  np.savetxt(os.path.join(DATA_PATH, 'skewedY.txt'), sampled_dist, delimiter='\t')

	  # Fourth case, skewed distribution in X and Y
	  cov[0,1] = 0.36
	  cov[1,0] = 0.36
	  sampled_dist = np.random.multivariate_normal(mean, cov, 100)
	  np.savetxt(os.path.join(DATA_PATH, 'skewedXY.txt'), sampled_dist, delimiter='\t')
	#+end_src

	#+RESULTS:
	: None

***                                                                :B_column:
	:PROPERTIES:
	:BEAMER_env: column
	:BEAMER_COL: 0.5
	:END:
	#+begin_export latex
	\begin{center}
		\begin{tikzpicture}[baseline,scale=0.6]
			\begin{axis}[
				enlargelimits=false,
				grid=major,
				ymin=-2.5, ymax=2.5,
				xmin=-2.5, xmax=2.5,
				xlabel=$x_1$,
				ylabel=$x_2$,
			]
			\addplot [only marks, mark=*,
			mark size=2.5pt, metropolisorange, mark options={fill=metropolisorange}] table {data/normal.txt};

			\draw [black!50] (axis cs:-2.5,0,0) -- (axis cs:2.5,0,0);
			\draw [black!50] (axis cs:0,-2.5,0) -- (axis cs:0,2.5,0);

			\end{axis}
		\end{tikzpicture}
	\end{center}
	#+end_export
** Covariance Matrix : Example 2
  \[ \bv{C} = \begin{bmatrix} 1 & 0 \\ 0 & \spot{0.2}\end{bmatrix}\]
*** \( \mathcal{N}(\gv{0}, \bv{I})\)                               :B_column:
	:PROPERTIES:
	:BEAMER_env: column
	:BEAMER_COL: 0.5
	:END:
	#+begin_export latex
	\begin{center}
		\begin{tikzpicture}[
			declare function={mu1=0;},
			declare function={mu2=0;},
			declare function={sigma1=1;},
			declare function={sigma2=0.2;},
			declare function={normal(\m,\s)=1/(2*\s*sqrt(pi))*exp(-(x-\m)^2/(2*\s^2));},
			declare function={bivar(\ma,\sa,\mb,\sb)=
				1/(2*pi*\sa*\sb) * exp(-((x-\ma)^2/\sa^2 + (y-\mb)^2/\sb^2))/2;}, scale=0.6, baseline]
			\begin{axis}[
				colormap name=whitered,
				view={45}{65},
				enlargelimits=false,
				grid=major,
				domain=-2.5:2.5,
				y domain=-2.5:2.5,
				samples=26,
				xlabel=$x_1$,
				ylabel=$x_2$,
				zlabel={$\mathcal{N}$},
				colorbar,
			]
			\addplot3 [surf] {bivar(mu1,sigma1,mu2,sigma2)};
			\addplot3 [domain=-2.5:2.5,samples=31, samples y=0, thick, smooth] (x,2.5,{normal(mu1,sigma1)});
			\addplot3 [domain=-2.5:2.5,samples=31, samples y=0, thick, smooth] (-2.5,x,{normal(mu2,sigma2)});

			\draw [black!50] (axis cs:-2.5,0,0) -- (axis cs:2.5,0,0);
			\draw [black!50] (axis cs:0,-2.5,0) -- (axis cs:0,2.5,0);

			\end{axis}
		\end{tikzpicture}
	\end{center}
	#+end_export

***                                                                :B_column:
	:PROPERTIES:
	:BEAMER_env: column
	:BEAMER_COL: 0.5
	:END:
	#+begin_export latex
	\begin{center}
		\begin{tikzpicture}[baseline,scale=0.6]
			\begin{axis}[
				enlargelimits=false,
				grid=major,
				ymin=-2.5, ymax=2.5,
				xmin=-2.5, xmax=2.5,
				xlabel=$x_1$,
				ylabel=$x_2$,
			]
			\addplot [only marks, mark=*,
			mark size=2.5pt, metropolisorange, mark options={fill=metropolisorange}] table {data/skewedX.txt};

			\draw [black!50] (axis cs:-2.5,0,0) -- (axis cs:2.5,0,0);
			\draw [black!50] (axis cs:0,-2.5,0) -- (axis cs:0,2.5,0);

			\end{axis}
		\end{tikzpicture}
	\end{center}
	#+end_export
** Covariance Matrix : Example 3
  \[ \bv{C} = \begin{bmatrix} \spot{0.2} & 0 \\ 0 & 1\end{bmatrix}\]
*** \( \mathcal{N}(\gv{0}, \bv{I})\)                               :B_column:
	:PROPERTIES:
	:BEAMER_env: column
	:BEAMER_COL: 0.5
	:END:
	#+begin_export latex
	\begin{center}
		\begin{tikzpicture}[
			declare function={mu1=0;},
			declare function={mu2=0;},
			declare function={sigma1=0.2;},
			declare function={sigma2=1.0;},
			declare function={normal(\m,\s)=1/(2*\s*sqrt(pi))*exp(-(x-\m)^2/(2*\s^2));},
			declare function={bivar(\ma,\sa,\mb,\sb)=
				1/(2*pi*\sa*\sb) * exp(-((x-\ma)^2/\sa^2 + (y-\mb)^2/\sb^2))/2;}, scale=0.6, baseline]
			\begin{axis}[
				colormap name=whitered,
				view={45}{65},
				enlargelimits=false,
				grid=major,
				domain=-2.5:2.5,
				y domain=-2.5:2.5,
				samples=26,
				xlabel=$x_1$,
				ylabel=$x_2$,
				zlabel={$\mathcal{N}$},
				colorbar,
			]
			\addplot3 [surf] {bivar(mu1,sigma1,mu2,sigma2)};
			\addplot3 [domain=-2.5:2.5,samples=31, samples y=0, thick, smooth] (x,2.5,{normal(mu1,sigma1)});
			\addplot3 [domain=-2.5:2.5,samples=31, samples y=0, thick, smooth] (-2.5,x,{normal(mu2,sigma2)});

			\draw [black!50] (axis cs:-2.5,0,0) -- (axis cs:2.5,0,0);
			\draw [black!50] (axis cs:0,-2.5,0) -- (axis cs:0,2.5,0);

			\end{axis}
		\end{tikzpicture}
	\end{center}
	#+end_export

***                                                                :B_column:
	:PROPERTIES:
	:BEAMER_env: column
	:BEAMER_COL: 0.5
	:END:
	#+begin_export latex
	\begin{center}
		\begin{tikzpicture}[baseline,scale=0.6]
			\begin{axis}[
				enlargelimits=false,
				grid=major,
				ymin=-2.5, ymax=2.5,
				xmin=-2.5, xmax=2.5,
				xlabel=$x_1$,
				ylabel=$x_2$,
			]
			\addplot [only marks, mark=*,
			mark size=2.5pt, metropolisorange, mark options={fill=metropolisorange}] table {data/skewedY.txt};

			\draw [black!50] (axis cs:-2.5,0,0) -- (axis cs:2.5,0,0);
			\draw [black!50] (axis cs:0,-2.5,0) -- (axis cs:0,2.5,0);

			\end{axis}
		\end{tikzpicture}
	\end{center}
	#+end_export
** Covariance Matrix : Example 4
  \[ \bv{C} = \begin{bmatrix} 0.2 & \spot{0.36} \\ \spot{0.36} & 1\end{bmatrix}\]
*** \( \mathcal{N}(\gv{0}, \bv{I})\)                               :B_column:
	:PROPERTIES:
	:BEAMER_env: column
	:BEAMER_COL: 0.5
	:END:
	#+begin_export latex
	\begin{center}
		\begin{tikzpicture}[
			declare function={mu1=0;},
			declare function={mu2=0;},
			declare function={sigma1=0.2;},
			declare function={sigma2=1.0;},
			declare function={normal(\m,\s)=1/(2*\s*sqrt(pi))*exp(-(x-\m)^2/(2*\s^2));},
			declare function={bivar(\ma,\sa,\mb,\sb)=
				1/(2*pi*\sa*\sb) * exp(-(((-0.35826*x - 0.93362*y)-\ma)^2/\sa^2 + ((-0.93362*x + 0.358266*y)-\mb)^2/\sb^2))/2;}, scale=0.6, baseline]
			\begin{axis}[
				colormap name=whitered,
				view={45}{65},
				enlargelimits=false,
				grid=major,
				domain=-2.5:2.5,
				y domain=-2.5:2.5,
				samples=26,
				xlabel=$x_1$,
				ylabel=$x_2$,
				zlabel={$\mathcal{N}$},
				colorbar,
			]
			\addplot3 [surf] {bivar(mu1,sigma1,mu2,sigma2)};
			%\addplot3 [domain=-2.5:2.5,samples=31, samples y=0, thick, smooth] (x,2.5,{normal(mu1,sigma1)});
			%\addplot3 [domain=-2.5:2.5,samples=31, samples y=0, thick, smooth] (-2.5,x,{normal(mu2,sigma2)});

			\draw [black!50] (axis cs:-2.5,0,0) -- (axis cs:2.5,0,0);
			\draw [black!50] (axis cs:0,-2.5,0) -- (axis cs:0,2.5,0);

			\end{axis}
		\end{tikzpicture}
	\end{center}
	#+end_export

***                                                                :B_column:
	:PROPERTIES:
	:BEAMER_env: column
	:BEAMER_COL: 0.5
	:END:
	#+begin_export latex
	\begin{center}
		\begin{tikzpicture}[baseline,scale=0.6]
			\begin{axis}[
				enlargelimits=false,
				grid=major,
				ymin=-2.5, ymax=2.5,
				xmin=-2.5, xmax=2.5,
				xlabel=$x_1$,
				ylabel=$x_2$,
			]
			\addplot [only marks, mark=*,
			mark size=2.5pt, metropolisorange, mark options={fill=metropolisorange}] table {data/skewedXY.txt};

			\draw [black!50] (axis cs:-2.5,0,0) -- (axis cs:2.5,0,0);
			\draw [black!50] (axis cs:0,-2.5,0) -- (axis cs:0,2.5,0);

			\end{axis}
		\end{tikzpicture}
	\end{center}
	#+end_export
** Covariance Matrix : Definition
   Covariance Matrix decides the "direction" of the population...

   # Code below taken from Pg 18 of
   # http://ctan.math.washington.edu/tex-archive/macros/latex/contrib/nicematrix/nicematrix.pdf
   \[ \left(\,\begin{NiceArray}{>{\strut}CCCC}%
   [create-extra-nodes,left-margin,right-margin,
   code-after = {\tikz \path [name suffix = -large,
   fill = metropolisorange!40,
   blend mode = multiply]
   (1-2.north west)|- (2-3.north west)|- (3-4.north west)|-
   (3-4.south east)|- (1-2.north west) ;
   \tikz \path [name suffix = -large,
   fill = metropolisorange!40,
   blend mode = multiply]
   (2-1.north west)|- (4-1.south west)|- (4-3.south east)|-
   (4-3.north west)|- (3-2.north west)|- (2-1.north west) ;
   \tikz \path [name suffix = -large,
   fill = royalblue!60,
   blend mode = multiply]
   (1-1.north west)|- (2-2.north west)|- (3-3.north west)|-
   (4-4.north west)|- (4-4.south east)|- (4-4.north west)|-
   (3-3.north west)|- (2-2.north west)|- (1-1.north west);} ]
   C_{11} & C_{12} & C_{13} & C_{14} \\C_{21} & C_{22} & C_{23} & C_{24} \\C_{31} & C_{32} & C_{33} & C_{34} \\C_{41} & C_{42} & C_{43} & C_{44}\end{NiceArray}\,
   \right)\]

   \tikz{\draw[fill=royalblue!60,line width=1pt]  rectangle(4ex, 2ex);}  \rightarrow Variance

   \tikz{\draw[fill=metropolisorange!40,line width=1pt]  rectangle(4ex, 2ex);}  \rightarrow Covariance

   Nonzero covariances \Rightarrow Population is not iid and is skewed wrt
   coordinate axes.

** Variance
*** Variance                                                   :B_definition:
	:PROPERTIES:
	:BEAMER_env: definition
	:END:
	\( \sigma^2 \) Is a measure of how "far" a variable changes away from its mean.

***                                                         :B_ignoreheading:
	:PROPERTIES:
	:BEAMER_env: ignoreheading
	:END:
	\[ \sigma^2  = \frac{1}{N} \sum_{i=1}^{N}(x_i - \mu)^2 \]
	where \(\mu \) is the mean and \( N \) is the number of samples.

** Variance
   #+begin_src python :exports none
	 import numpy as np
	 import os

	 DATA_PATH = './data/'
	 if not os.path.isdir(DATA_PATH):
		 os.makedirs(DATA_PATH)

	 # Generate data for high and low variance cases

	 # First case, normal distribution centered at origin
	 sampled_dist = np.random.standard_normal(100)
	 x_axis = np.arange(100)
	 # Low variance data
	 np.savetxt(os.path.join(DATA_PATH, 'lowvar.txt'), np.c_[x_axis, 0.3*sampled_dist], delimiter='\t')
	 # high variance data
	 np.savetxt(os.path.join(DATA_PATH, 'highvar.txt'), np.c_[x_axis, 1.5*sampled_dist], delimiter='\t')
   #+end_src

   #+RESULTS:
   : None

*** Low variance                                                  :B_example:
	:PROPERTIES:
	:BEAMER_env: example
	:END:
	#+begin_export latex
	\begin{center}
		\begin{tikzpicture}[baseline,scale=0.9]
			\begin{axis}[
				% only scale the axis, not the axis including the ticks and labels
				scale only axis=true,
				% set `width' and `height' to the desired values
				width=0.7\textwidth,
				height=0.2\textwidth,
				enlargelimits=true,
				grid=major,
				xlabel=$i$,
				ylabel=$x_i$,
				ymin=-2.5,ymax=2.5,
				xticklabels={,,},
			]
			\addplot [only marks, mark=*,
			mark size=2.5pt, metropolisorange, mark options={fill=metropolisorange}] table {data/lowvar.txt};
			\draw[thick, black] (axis cs:\pgfkeysvalueof{/pgfplots/xmin},0) -- (axis cs:\pgfkeysvalueof{/pgfplots/xmax},0) node[left,pos=1] (endofplot){};
			\node [above] at (endofplot) {$\mu$};
			\end{axis}
		\end{tikzpicture}
	\end{center}
	#+end_export

*** High variance                                                 :B_example:
	:PROPERTIES:
	:BEAMER_env: example
	:END:
	#+begin_export latex
	\begin{center}
		\begin{tikzpicture}[baseline,scale=0.9]
			\begin{axis}[
				% only scale the axis, not the axis including the ticks and labels
				scale only axis=true,
				% set `width' and `height' to the desired values
				width=0.7\textwidth,
				height=0.2\textwidth,
				enlargelimits=true,
				grid=major,
				xlabel=$i$,
				ylabel=$x_i$,
				ymin=-2.5,ymax=2.5,
				xticklabels={,,},
			]
			\addplot [only marks, mark=*,
			mark size=2.5pt, metropolisorange, mark options={fill=metropolisorange}] table {data/highvar.txt};
			\draw[thick, black] (axis cs:\pgfkeysvalueof{/pgfplots/xmin},0) -- (axis cs:\pgfkeysvalueof{/pgfplots/xmax},0) node[left,pos=1] (endofplot){};
			\node [above] at (endofplot) {$\mu$};
			\end{axis}
		\end{tikzpicture}
	\end{center}
	#+end_export
** Covariance
*** Covariance                                                 :B_definition:
	:PROPERTIES:
	:BEAMER_env: definition
	:END:
	covar\((x,y) \) Is a measure of how two variables change with one another.

***                                                         :B_ignoreheading:
	:PROPERTIES:
	:BEAMER_env: ignoreheading
	:END:
	\[ \sigma_{xy} = \frac{1}{N} \sum_{i=1}^{N}(x_i - \mu_x)(y_i - \mu_y) \]
	where \(\mu_x, \mu_y\) are the respective means and \( N \) is the number of samples.

** Covariance
   #+begin_src python :exports none
	 import numpy as np
	 import os

	 DATA_PATH = './data/'
	 if not os.path.isdir(DATA_PATH):
		 os.makedirs(DATA_PATH)

	 # Generate data for positive and negative covariant datasets

	 mean = np.zeros((3,))
	 covar = np.eye(3)
	 covar[0,1] = 10
	 covar[1,0] = covar[0,1]
	 covar[0,2] = -10
	 covar[2,0] = covar[0,2]

	 # First case, normal distribution centered at origin
	 N_SAMPLES = 20
	 sampled_dist = 0.2 * np.random.multivariate_normal(mean, covar, N_SAMPLES)
	 x_axis = np.arange(N_SAMPLES)

	 # Original variance data
	 np.savetxt(os.path.join(DATA_PATH, 'orig.txt'), np.c_[x_axis, sampled_dist[:, 0]], delimiter='\t')

	 # Positive variance data
	 np.savetxt(os.path.join(DATA_PATH, 'poscovar.txt'), np.c_[x_axis, sampled_dist[:, 1]], delimiter='\t')
	 # Negative covariance data
	 np.savetxt(os.path.join(DATA_PATH, 'negcovar.txt'), np.c_[x_axis, sampled_dist[:, 2]], delimiter='\t')
   #+end_src

   #+RESULTS:
   : None

*** Covariance (x,y,z)                                            :B_example:
	:PROPERTIES:
	:BEAMER_env: example
	:END:
	#+begin_export latex
	\begin{center}
		\begin{tikzpicture}[baseline]
			\begin{axis}[
				% only scale the axis, not the axis including the ticks and labels
				scale only axis=true,
				% set `width' and `height' to the desired values
				width=0.7\textwidth,
				height=0.12\textwidth,
				enlargelimits=true,
				grid=major,
				ylabel=$x_i$,
				ymin=-1.0,ymax=1.0,
				xticklabels={,,},
			]
			\addplot [only marks, mark=*,
			mark size=2.5pt, metropolisorange, mark options={fill=metropolisorange}] table {data/orig.txt};
			\draw[thick, black] (axis cs:\pgfkeysvalueof{/pgfplots/xmin},0) -- (axis cs:\pgfkeysvalueof{/pgfplots/xmax},0) node[left,pos=1] (endofplot){};
			\node [above] at (endofplot) {$\mu_x$};
			\end{axis}
		\end{tikzpicture}
	\end{center}

	\begin{center}
		\begin{tikzpicture}[baseline]
			\begin{axis}[
				% only scale the axis, not the axis including the ticks and labels
				scale only axis=true,
				% set `width' and `height' to the desired values
				width=0.7\textwidth,
				height=0.12\textwidth,
				enlargelimits=true,
				grid=major,
				ylabel=$y_i$,
				ymin=-1.0,ymax=1.0,
				xticklabels={,,},
			]
			\addplot [only marks, mark=*,
			mark size=2.5pt, metropolisorange, mark options={fill=metropolisorange}] table {data/poscovar.txt};
			\draw[thick, black] (axis cs:\pgfkeysvalueof{/pgfplots/xmin},0) -- (axis cs:\pgfkeysvalueof{/pgfplots/xmax},0) node[left,pos=1] (endofplot){};
			\node [above] at (endofplot) {$\mu_y$};
			\end{axis}
		\end{tikzpicture}
	\end{center}

	\begin{center}
		\begin{tikzpicture}[baseline]
			\begin{axis}[
				% only scale the axis, not the axis including the ticks and labels
				scale only axis=true,
				% set `width' and `height' to the desired values
				width=0.7\textwidth,
				height=0.12\textwidth,
				enlargelimits=true,
				grid=major,
				xlabel=$i$,
				ylabel=$z_i$,
				ymin=-1.0,ymax=1.0,
				xticklabels={,,},
			]
			\addplot [only marks, mark=*,
			mark size=2.5pt, metropolisorange, mark options={fill=metropolisorange}] table {data/negcovar.txt};
			\draw[thick, black] (axis cs:\pgfkeysvalueof{/pgfplots/xmin},0) -- (axis cs:\pgfkeysvalueof{/pgfplots/xmax},0) node[left,pos=1] (endofplot){};
			\node [above] at (endofplot) {$\mu_z$};
			\end{axis}
		\end{tikzpicture}
	\end{center}
	#+end_export


  # #+begin_export latex
  # \includegraphics[page=77,width=1.0\textwidth]{images/cma_slideshare.pdf}
  # #+end_export
** Covariance Matrix : Definition
   Covariance Matrix decides the "direction" of the population...

   # Code below taken from Pg 18 of
   # http://ctan.math.washington.edu/tex-archive/macros/latex/contrib/nicematrix/nicematrix.pdf
   \[ \left(\,\begin{NiceArray}{>{\strut}CCCC}%
   [create-extra-nodes,left-margin,right-margin,
   code-after = {\tikz \path [name suffix = -large,
   fill = metropolisorange!40,
   blend mode = multiply]
   (1-2.north west)|- (2-3.north west)|- (3-4.north west)|-
   (3-4.south east)|- (1-2.north west) ;
   \tikz \path [name suffix = -large,
   fill = metropolisorange!40,
   blend mode = multiply]
   (2-1.north west)|- (4-1.south west)|- (4-3.south east)|-
   (4-3.north west)|- (3-2.north west)|- (2-1.north west) ;
   \tikz \path [name suffix = -large,
   fill = royalblue!60,
   blend mode = multiply]
   (1-1.north west)|- (2-2.north west)|- (3-3.north west)|-
   (4-4.north west)|- (4-4.south east)|- (4-4.north west)|-
   (3-3.north west)|- (2-2.north west)|- (1-1.north west);} ]
   C_{11} & C_{12} & C_{13} & C_{14} \\C_{21} & C_{22} & C_{23} & C_{24} \\C_{31} & C_{32} & C_{33} & C_{34} \\C_{41} & C_{42} & C_{43} & C_{44}\end{NiceArray}\,
   \right)\]

   \tikz{\draw[fill=royalblue!60,line width=1pt]  rectangle(4ex, 2ex);}
   \rightarrow Variance
   \tikz{\draw[fill=metropolisorange!40,line width=1pt]  rectangle(4ex, 2ex);}
   \rightarrow Covariance

   \( C_{12} \) \Rightarrow \( \sigma_{12} \),  \( C_{23} \) \Rightarrow \( \sigma_{23} \), \cdots

** Sampling : Python--Answers
	* What is \( \bv{C}\) / ~cov~ (in a 2D case) and its meaning?
	  1. Covariance, how a gene varies with another (across dimensions)
	  2. \( \mu_x = \frac{1}{N} \sum_{i=1}^{N}x_i \)
	  3. \( \sigma_x^2 = \frac{1}{N} \sum_{i=1}^{N}(x_i - \mu_x)^2 \) and \(
		 \sigma_{xy} = \frac{1}{N} \sum_{i=1}^{N}(x_i - \mu_x)(y_i - \mu_y) \)
	* ~cov~ needs to be SPD. Is it? What about the update step?
	  1. Symmetric by definition
	  2. Symmetric after update too
	* What happens in ~numpy~ if it is not?
	  1. ~numpy~ checks for PD, else throws an exception
***                                                                  :B_note:
	:PROPERTIES:
	:BEAMER_env: note
	:END:
	- Return back to the top and discuss what's C and stuff.

** Sampling : Idea of \( \bv{C} \) \rightarrow math
*** What is CMA-ES doing?
	- How does CMA estimate \( \bv{C} \)?
	- What about the choice of weights?
	- What is CMA doing by adapting \( \bv{C}\)?
***                                                                  :B_note:
	:PROPERTIES:
	:BEAMER_env: note
	:END:
	- \( \mu_x^{(g+1)} = \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}x_i \) and \(
      \sigma_x^{2, (g+1)} = \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}(x_i -
      \mu_x^{(g)})^2 \)
	- This is rank \( \mu \) update, but with also exponential weighting of
      previous \( C \) (show separately, will be discussed in CMA)
	- Choice of weights reflect "normalization"
	- Conducts PCA (eigenvectors), rotated representation \( \bv{C} =
	  \bv{B}\bv{D}^2\bv{D}^T \), inverse Hessian
	  (second order)
** Sampling : Idea of \( \bv{C} \) \rightarrow math--Answers
	- How does CMA estimate \( \bv{C} \)?
	  1. You can use the new population to get \( \bv{C} \) too, but information
         is lost (no information on how the population "evolved", see EMNA from
         previous slides)
	  2. *Idea* : Use \(  \mu_x^{(g+1)} = \frac{1}{N_{best}}
         \sum_{i=1}^{N_{best}}x_i \) rather than \(\sigma_x^{2, (g+1)} =
         \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}(x_i - \mu_x^{(g)})^2 \),
         across \( N_{best}\) individuals to estimate covariance between genes
         (rank \( \mu \) update)
	  3. Exponential weighting, discussed later on
	- What about the choice of weights?
	  1. Reflect normalization (relates back to the ability of CMA to maintain invariance)
	- What is CMA doing by adapting \( \bv{C}\)?
	  1. Conducts PCA (eigenvectors), rotated representation \( \bv{C} =
		 \bv{B}\bv{D}^2\bv{B}^T \), inverse Hessian
		 (second order)
** PCA
*** CMAes performs PCA on the optimization data
*** PCA?
	1) Principal Component Analysis
	2) Find directions with
	   - High Variance
	   - Low Covariance with other components
	3) Find dimensions that are "independent" from one another
	4) Gives a useful basis (in this case for \( \bv{C}\) )
** Sampling : Parameters
*** Choice of \( \lambda \)?
	- Look at the CMA tutorial : [[file:~/Desktop/Masters_Resources/readings/optimization/Hansen/The%20CMA%20evolution%20strategy%20A%20tutorial.pdf][The CMA tutorial]]/[[https://arxiv.org/pdf/1604.00772.pdf][CMA tutorial on Arxiv]]
	- Usually \( \lambda = \lfloor 4 + 3 \ln n \rfloor \)
	- And \( \mu = \lfloor \lambda/ 2 \rfloor \)
** Second step : Selection
*** How to select \( \mu \) best individuals
   - \( \langle \gv{z}_k \rangle_{w} = \sum_{i=1}^{\mu} w_i \gv{z}_{i:\lambda} \)
   - Constraint on weights: \( \sum_{i=1}^{\mu} w_i = 1, \; w_i > 0 \; \forall i=1
     \cdots \mu \) (at least in our version of CMA)
*** Computing?
   - Fitness function evaluation left upto user (including constraints etc.).
     This determines the \( \mu \) best individuals.
   - The weighted sum can be evaluated using ~np.inner()/broadcasting with
     */np.sum() after *~ ...
** Selection : Parameters
*** Choice of \( w_i \)?
	- Look at the CMA tutorial : [[file:~/Desktop/Masters_Resources/readings/optimization/Hansen/The%20CMA%20evolution%20strategy%20A%20tutorial.pdf][The CMA tutorial]]/[[https://arxiv.org/pdf/1604.00772.pdf][CMA tutorial on Arxiv]]
** Third step : Recombination
*** Recombination to get new \( m \)
   - \( \gv{m} \leftarrow \gv{m} + \sigma \langle \gv{z} \rangle_{w} \)
   - No parameters in this step!
*** Notice!                                                    :B_alertblock:
	:PROPERTIES:
	:BEAMER_env: alertblock
	:END:
  - \( \sigma \) is the "overall" step size and is a scalar.
  - It could also be a matrix. Is this a good idea?
	- What about a diagonal matrix?
*** Computing?
   - Use elementwise addition using ~+~ operator
***                                                                  :B_note:
	:PROPERTIES:
	:BEAMER_env: note
	:END:
	- Having \( \sigma \) as matrix is not a good idea because one dimension
      depends on another---a which complicates stuff for a black box algorithm.
	- Besides that's precisely what the \( \bv{C} \) encodes---both rotation and
      scaling.
	- So choose only a scalar.
** Third step : Recombination--Answers
  - \(\sigma\) could also be a matrix. Is this a good idea?
	- *NO*!
	- One dimension depends on another, but not during sampling. This degrades
      the convergence of the algorithm
  - What about a diagonal matrix?
	- *NO*!
	- \bv{C}= \bv{BD^2B^T} does the job of maintaining scaling, orientation etc. of the elements.

  *CONCLUSION*---Scalar \(\sigma\) is apt.
** Fourth step : Step size control
*** Control for \( \sigma \) and cumulation \(\gv{p}_{\sigma} \)
   - \( \gv{p}_\sigma \leftarrow (1 - c_\sigma) \gv{p}_\sigma +
     \sqrt{c_\sigma \left( 2 - c_\sigma \right)} \mu_{\text{cov} }
     \bv{C}^{-\frac{1}{2}} \langle \gv{z} \rangle_{w} \)
   - \( \sigma \leftarrow \sigma \exp{\left( \frac{c_\sigma}{d_\sigma} \left[
     \tfrac{ \norm{\gv{p}_\sigma} }{ \mathsf{E} \norm{ \mathcal{N}\left( {0},
     \bv{I} \right) } } - 1 \right] \right)} \)

*** Computing/Python?
   + Notice you need to invert the covariance matrix! How will you do it?
	 + *Hint*: Exploit properties of \bv{C}!
	 + This means you just need ~np.linalg.eigh()~ for now (there are many other
       powerful methods for general symmetric matrix inverse)
	 + Can reduce \( \order{n^3}\) to \(\order{n^2} \) in practice? ( See B2.
       Strategy internal numerical effort in CMA tutorial)

***                                                                  :B_note:
	:PROPERTIES:
	:BEAMER_env: note
	:END:
	- Positive definiteness is the property. Show a demo of how positive
      definiteness used to invert.
	- Spectral theorem : Symmetric matrices have a complete set of eigenvectors
      (no generalized EV needed)
	- PD : All positive eigenvalues needed
** Step size control: Computing/Python
*** Computing continued
   + Extensive use of matvecs (~@~)
   + What about the norm in the \( \sigma \) update?
	 + What is a norm?
	 + So what norm should we use?
	   + The two-norm is widely used (Euclidean distance)
   + What's \( \mathsf{E} \)?
	 - What's \( \mathsf{E} \norm{ \mathcal{N}\left( {0}, \bv{I} \right) } \)?
	   - \( \approx \sqrt{n} \left( 1 - \tfrac{1}{4n} + \tfrac{1}{21n^2} \right) \)
***                                                                  :B_note:
	:PROPERTIES:
	:BEAMER_env: note
	:END:
	- Expected length of distribution
** Step size control \rightarrow math
*** What is path update doing?
	- Increase probability of reproducing successful solution paths...
	- Weighting with exponential decay...
	- What about the choice of weights?
	  - Makes the expected length independent of the direction
	  - "Follows" the random choice of \( \gv{p}^{(0)}_\sigma\)
*** What is \(\sigma\) update doing?
	- Decrease/Increase size until path steps are uncorrelated...
	- How does the two norm of the path reflect this "un"correlation?
	- What about the choice of weights?
***                                                                  :B_note:
	:PROPERTIES:
	:BEAMER_env: note
	:END:
	- Two norm weighted by expectation tells you how much deviation is there in
      the expectation...
** Step size : Parameters
*** Choice of \( c_\sigma , d_\sigma \)?
	- Look at the CMA tutorial : [[file:~/Desktop/Masters_Resources/readings/optimization/Hansen/The%20CMA%20evolution%20strategy%20A%20tutorial.pdf][The CMA tutorial]]/[[https://arxiv.org/pdf/1604.00772.pdf][CMA tutorial on Arxiv]]
	- \( c_\sigma \) is learning rate for cumulation usually set to \( \approx
      \tfrac{4}{n} \)
	- \( d_\sigma \) is the damping parameter for step size update \(\approx 1 + \frac{\mu_{\text{cov}}}{\sqrt{n}} \)
	# \[ \scalebox{1.5}{$\tfrac{\mu_{ \text{cov} } + 2 }{ n + 5 + \mu_{ \text{cov} }
	# }$} \]
** Fifth step : Covariance matrix adaptation
*** Control for \( \bv{C} \) and cumulation \(\gv{p}_{c} \)
   - \( \gv{p}_c \leftarrow (1 - c_c) \gv{p}_c+
     \sqrt{c_c\left( 2 - c_c\right) } \mu_{\text{cov}}
     \langle \gv{z} \rangle_{w} \)
   - \( \bv{C} \leftarrow (1 - c_{\text{cov}}) \bv{C} +
     \frac{c_{\text{cov}}}{\mu_{\text{cov}}} \gv{p}_{c} \gv{p}^T_c +
     c_{\text{cov}} \left( 1 - \frac{1}{\mu_{cov}}\right) \bv{Z}  \)
	 where \( \bv{Z} =  \sum_{i=1}^{\mu} w_i \gv{z}_{i:\lambda} \gv{z}^T_{i:\lambda}\)
*** Computing/Python?
   + Usual operations (~*,+~)
   + For calculating outer products, use ~np.outer()~

** CMA \rightarrow math
*** What is cumulation for \(\gv{p}_c\) doing?
	- Weighting with exponential decay for prior values
	- New information from PCA of steps updated into \( \bv{C} \) path
	- What about the choice of weights?
*** What is \(\bv{C}\) update doing?
	- Weighting with exponential decay for prior values
	- Rank one update using \( \gv{p}_c \) (What's *rank*?)
	  - Why is the update rank one? (One-dimensional information)
	  - Why use \( \gv{p}_c \) rather than \( \langle z \rangle\)?
	- Rank \( \mu \) update
	  - As seen earlier, CMA cleverly estimates \( \bv{C} \) using old step
        information
***                                                                  :B_note:
	:PROPERTIES:
	:BEAMER_env: note
	:END:
	- Rank demo using \( [1 2 3] \)
	- using \( \langle z \rangle\) loses information about correlation between
      steps, the history informatino (Explain that this may lead to effects on
      path length adaption and so on)

** CMA: Parameters
*** Choice of \( c_c, c_{\text{cov}}\)?
	- Look at the CMA tutorial : [[file:~/Desktop/Masters_Resources/readings/optimization/Hansen/The%20CMA%20evolution%20strategy%20A%20tutorial.pdf][The CMA tutorial]]/[[https://arxiv.org/pdf/1604.00772.pdf][CMA tutorial on Arxiv]]
	- \( c_c\) is learning rate for path cumulation set to \( \approx
      \tfrac{4}{n} \)
	- \( c_{\text{cov}} \approx \tfrac{2 + \mu^2_{\text{cov}}}{n^2} \)

** Terminating CMA
*** Algorithm should be stopped when CPU-time is wasted. Then we can:
	  1) restart (eventually with increased population size)
	  2) reconsider encoding and objective function formulation
*** Problem independent
	- ~NoEffectAxis~ : Stop if adding \(0.1\) std.dev. vector to any direction
       of basis \( \bv{B} \) does not change \( \gv{m} \)
	- ~NoEffectCoord~ : Stop if adding \(0.2\) std.dev. to any coordinate does not change \( \gv{m} \)
	- ~ConditionCov~: stop if condition number of covariance matrix exceeds \(
      10^{14} \)
	  - Whats condition number of a matrix?
	  - ~np.linalg.cond()~, although you can directly check ~D~
***                                                                  :B_note:
	:PROPERTIES:
	:BEAMER_env: note
	:END:
	- First criteria is explanatory: when the c matrix is small, it will still
      choose yours as an optima
	- ~NoEffectCoord~ : \( m_i = m_i + 0.2 \sigma c_i \)
	- using \( \langle z \rangle\) loses information about correlation between
      steps, the history informatino (Explain that this may lead to effects on
      path length adaption and so on)
	- Condition number tells you stretch of matrix. If 10^14 you can go home.

** Terminating CMA contd.
*** Problem independent
  	- ~EqualFunValues~: stop if the range of the best \( \gv{f}(\gv{x}) \) of
      the last \( 10 + \lceil 30n/\lambda \rceil \) generations is zero.
	- ~Stagnation~: Track history of the best and the median fitness in each
      iteration over the last \( 20 \%\) but at least \( 120+30n/\lambda \) and
      no more than \( 20000\) iterations. Stop, if in both histories the median
      of the last (most recent) \( 30 \% \) values is not better than the median
      of the first \( 30\%\).
	- ~TolXUp~: stop if \( \sigma \times max(diag(\bv{D})) \) increased by more
      than \( 10^4\). This indicates a far too small initial \( \sigma \), or
      divergence.
***                                                         :B_ignoreheading:
	:PROPERTIES:
	:BEAMER_env: ignoreheading
	:END:
	*We note that there are problem dependent diagnostics too!*
***                                                                  :B_note:
	:PROPERTIES:
	:BEAMER_env: note
	:END:
	- Equalfunvalus is self explanatory
	- Average properties of the simualtion does not improve
	- Toelrance limit reached

** Boundaries/Constraints in CMA : Best solution strictly inside
	- Set fitness (for minimization problem) as
\[ f_{\text{fitness}} (\gv{x}) = f_{\text{max}} + \norm{\gv{x} -
\gv{x}_{\text{feasible}}} \]
	    1. Notation
		   1) \( f_{\text{max}} \) is larger than worst feasible fitness
		   2) \( \gv{x}_{\text{feasible}} \) is constant,in the middle of feasible region
	    2. Caveat : Optimal solution not too close to the infeasible region


	- Alternatively, resample any infeasible point until it becomes feasible

** Boundaries/Constraints in CMA : Repair
	- Simply "repair" infeasible individuals (say when boundary is a box) before
      update so that they satisfy the constraint
	  1. Caveat : Repairs are dangerous
		 - Distribution affected by repair, hurting CMA's convergence
	  2. "Re-repair" mechanisms to prevent divergence are also reported

	- Alternatively, penalize the repaired solutions
	\[  f_{\text{fitness}} (\gv{x}) = f(\gv{x}_{\text{repaired}}) + \alpha \norm{\gv{x} -
\gv{x}_{\text{repaired}}}^2 \]

* Some "realistic" examples
** Brachistochrone problem
*** Basd                                                     :B_column:BMCOL:
	:PROPERTIES:
	:BEAMER_env: column
	:BEAMER_col: 0.5
	:END:

	#+CAPTION: Johannes Bernoulli
	[[file:images/Johann_Bernoulli2.jpg]]
***                                                          :B_column:BMCOL:
	:PROPERTIES:
	:BEAMER_env: column
	:BEAMER_col: 0.5
	:END:
	 #+begin_export latex
	 \begin{center}
	   \begin{tikzpicture}[baseline,scale=1.2]
		 \draw [-latex] (-0.5, 0) -- (4, 0) node [right] {$x$};
		 \draw [-latex] (0, 0.5) -- (0, -2) node [below] {$y$};

		 %\node [circle,fill=black,inner sep=0pt,minimum size=3pt,label=below:{$\frac{3}{2}$}] (a) at (2/3,0) {};
		 \node [anchor = south east] (a) {$A$};

		 %\node at (3, -1) [circ] {};
		 \node at (3, -1) [right] (b) {$B$};
		 \draw [thick, black] (0, 0) parabola bend (2, -1.5) (3, -1);
		 \draw [thin, gray, dashed] (0, 0) -- (3, -1);
		 \draw [black, fill=black] circle [radius=0.05];
		 \draw [black, fill=black] (3,-1) circle [radius=0.05];
		 \draw [black, fill=metropolisorange] (0.86, -1) circle [radius=0.1];
		 \node at (2, -1.5) [below] {$y = f(x)$};
		 \draw [->] (3.8, -0.5) -- (3.8, -1.8) node [below] {$\gv{g}$};
	   \end{tikzpicture}
	 \end{center}
	#+end_export
	\( \beta \rho \acute{\alpha} \chi \iota \sigma \tau o \varsigma \)
	(/brachistos/ or shortest) \( \chi
	\rho \acute{o} \nu o \varsigma \) (/chronos/ or time)
** Brachistochrone problem
***                                                                   :BMCOL:
	:PROPERTIES:
	:BEAMER_col: 0.35
	:END:
	[[file:images/Johann_Bernoulli2.jpg]]
***                                                       :B_quotation:BMCOL:
	:PROPERTIES:
	:BEAMER_env: quotation
	:BEAMER_col: 0.7
	:END:
	 #+begin_export latex
	  {\small ``I, Johann Bernoulli, address the most brilliant mathematicians in the
	  world..... Following the example set by Pascal, Fermat, etc., I hope to
	  gain the gratitude of the whole scientific community by placing before the
	  finest mathematicians of our time a problem which will test their methods and
	  the strength of their intellect..."}

	  {\small ``...Given two points A and B in a vertical plane, what is the curve traced out by a
	  point acted on only by gravity, which starts at A and reaches B in the shortest time."}
	#+end_export

** Brachistochrone problem : more formally
***                                                         :B_ignoreheading:
	:PROPERTIES:
	:BEAMER_env: ignoreheading
	:END:
	 #+begin_export latex
	 \begin{center}
	   \begin{tikzpicture}[baseline, scale=0.8]
		 \draw [-latex] (-0.5, 0) -- (4, 0) node [right] {$x$};
		 \draw [-latex] (0, 0.5) -- (0, -2) node [below] {$y$};

		 %\node [circle,fill=black,inner sep=0pt,minimum size=3pt,label=below:{$\frac{3}{2}$}] (a) at (2/3,0) {};
		 \node [anchor = south east] (a) {$A$};

		 %\node at (3, -1) [circ] {};
		 \node at (3, -1) [right] (b) {$B$};
		 \draw [thick, black] (0, 0) parabola bend (2, -1.5) (3, -1);
		 \draw [thin, gray, dashed] (0, 0) -- (3, -1);
		 \draw [black, fill=black] circle [radius=0.05];
		 \draw [black, fill=black] (3,-1) circle [radius=0.05];
		 \draw [black, fill=metropolisorange] (0.86, -1) circle [radius=0.1];
		 \node at (2, -1.5) [below] {$y = f(x)$};
		 \draw [->] (3.8, -0.5) -- (3.8, -1.8) node [below] {$\gv{g}$};
	   \end{tikzpicture}
	 \end{center}
	#+end_export
***  Brachistochrone problem                                   :B_definition:
	:PROPERTIES:
	:BEAMER_env: definition
	:END:
	 For \( x : [x_A, x_B] \), let
	 \( \mathbf{x} \mapsto \begin{pmatrix} x \\ y(x) \end{pmatrix},
	 y : [0, x] \to \mathbb{R} \) with BoCos.
	 # \ni y(x_*) = y_* ; *=A,B\).

	 Find \( y^*  = \mathop{\mathrm{arg\,min}_y} L[y] \mathrel{\mathop:}= t_B(y, y',y'') \) where
	 \( t : \mathbb{R}^3 \to \mathbb{R}^+ \) is trajectory time.
**** In the context of optimization \( \left( \mathcal{C}^2, \mathbb{R}, f, \leq \right)\)
	 By solving \( \ddot{x} = \frac{g_x + y' \left[ g_y - y'' \dot{x}^2 \right]}{1 + y'^2} \)
	 with \(x(A) = x_A, \dot{x}(A) = 0\) to obtain \( x = f(t, y, y', y''') \).
	 Then \( t \mathrel{\mathop:}= f^{-1}(x, y, y', y''') \)
** Requirements
*** Simulation                                                 :B_alertblock:
	:PROPERTIES:
	:BEAMER_env: alertblock
	:END:
	 - *Numerical solution* of non-linear Ordinary Differential Equation (ODE)
*** Optimization problem                                       :B_alertblock:
	:PROPERTIES:
	:BEAMER_env: alertblock
	:END:
	 - *Representation* of \( \mathcal{C}^2 \)
	 - Definition of *fitness function*
	 - Black-box *optimizer* (we pick CMAes in this case)
	 - *Constraints* on domain and range
	   - not all \( \mathcal{C}^2 \) are feasible (boundary conditions!)
** ODE solver
*** Ordinary Differential Equation                             :B_definition:
	:PROPERTIES:
	:BEAMER_env: definition
	:END:
	\[ \frac{dy}{dt} = f(t, y, y', \dots, y^{k})\]
	accompanied by \(k\) initial conditions \Rightarrow solved by \( y(t) \).
   - Few non-linear problems can be solved analytically \Rightarrow *numerical methods*
   - /Timesteppers/---Euler, Runge-Kutta etc.
*** Scipy's ODE solver                                            :B_example:
	:PROPERTIES:
	:BEAMER_env: example
	:END:
	- ~scipy.integrate.odeint~ *DEMO*
	- many options ~RK23, RK45, BDF,...~
	- can specify error tolerances ~rtol~, ~atol~
** Representation
*** Function space \(\mathcal{C}^2 \to \mathbb{R}\)?
	- Our optimizer works ONLY with real numbers
*** Linear combination of basis functions                           :B_block:
	:PROPERTIES:
	:BEAMER_env: block
	:END:
	- \( \forall x \in D\), \(f(x) = \sum_{i=1}^{N} c_i \phi_i(x)\)
	- look for /good/ values of \(c_i\)
*** Polynomial                                                        :BMCOL:
	:PROPERTIES:
	:BEAMER_col: 0.32
	:END:
	 #+begin_export latex
	 \begin{center}
		 \begin{tikzpicture}[scale=0.48]
			 \begin{axis}[
						 title={Polynomial bases},
						 xmin=0,
						 xmax=1,
						 ymin=-1.05,
						 ymax=1.05,
						 samples=50,
						 xlabel={$s$},
						 ylabel={$\phi(s)$},
						 ylabel shift = -10 pt]
				  \addplot[royalblue,  ultra thick, domain=0:1] {x};
				  \addplot[scarlet, ultra thick, domain=0:1] {x^2};
				  \addplot[black,  ultra thick, domain=0:1] {x^3};
				  \addplot[metropolisorange,  ultra thick, domain=0:1] {x^4};
				  \addplot[shamrockgreen,  ultra thick, domain=0:1] {x^5};
				  \addplot[deeppink,  ultra thick, domain=0:1] {x^6};
				  \addplot[burple,  ultra thick, domain=0:1] {1};
				  \draw[ultra thin] (axis cs:\pgfkeysvalueof{/pgfplots/xmin},0) -- (axis cs:\pgfkeysvalueof{/pgfplots/xmax},0);
			 \end{axis}
		 \end{tikzpicture}
	 \end{center}
	#+end_export
*** Fourier                                                           :BMCOL:
	:PROPERTIES:
	:BEAMER_col: 0.32
	:END:
	 #+begin_export latex
	 \begin{center}
		 \begin{tikzpicture}[scale=0.48]
			 \begin{axis}[
						 title={Fourier bases},
						 xmin=0,
						 xmax=1,
						 ymin=-1.05,
						 ymax=1.05,
						 samples=50,
						 xlabel={$s$}]
				 \addplot[royalblue, ultra thick, domain=0:1] {sin(deg(pi * x))};
				 \addplot[scarlet, ultra thick, domain=0:1] {cos(deg(pi * x))};
				 \addplot[black,  ultra thick, domain=0:1] {sin(deg(2.0 * pi * x))};
				 \addplot[metropolisorange,  ultra thick, domain=0:1] {cos(deg(2.0 * pi * x)))};
				 \addplot[shamrockgreen,  ultra thick, domain=0:1] {sin(deg(3.0 * pi * x))};
				 \addplot[deeppink,  ultra thick, domain=0:1] {cos(deg(3.0 * pi * x)))};
				 \addplot[burple,  ultra thick, domain=0:1] {1};

				 \draw[ultra thin] (axis cs:\pgfkeysvalueof{/pgfplots/xmin},0) -- (axis cs:\pgfkeysvalueof{/pgfplots/xmax},0);
			 \end{axis}
		 \end{tikzpicture}
	 \end{center}
	#+end_export
*** BSplines                                                          :BMCOL:
	:PROPERTIES:
	:BEAMER_col: 0.32
	:END:
	 #+begin_export latex
	 \begin{center}
		 \begin{tikzpicture}[scale=0.48]
			 \begin{axis}[
						 title={B-splines},
						 xmin=0,
						 xmax=1,
						 ymin=-1.05,
						 ymax=1.05,
						 samples=50,
						 xlabel={$s$}]
					 % Taken from https://pages.mtu.edu/~shene/COURSES/cs3621/NOTES/spline/B-spline/bspline-ex-1.html
					 % N02
					 \addplot[royalblue, ultra thick, domain=0:0.3] {(1 - (10/3)*x)^2 };

					 % N12
					 \addplot[scarlet, ultra thick, domain=0:0.3] {(20/3)*(x - (8/3)*x^2)  };
					 \addplot[scarlet, ultra thick, domain=0.3:0.5] {2.5*(1.0 - 2*x)^2};

					 % N22
					 \addplot[black, ultra thick, domain=0:0.3] {(20/3)*x^2  };
					 \addplot[black, ultra thick, domain=0.3:0.5] {-3.75 + 25*x - 35*x^2};

					 % N32
					 \addplot[metropolisorange,  ultra thick, domain=0.3:0.5] {(5*x - 1.5)^2};
					 \addplot[metropolisorange,  ultra thick, domain=0.5:0.6] {(6 - 10 * x)^2};

					 % N42
					 \addplot[shamrockgreen,  ultra thick, domain=0.5:0.6] {20 * (-2 + 7*x - 6*x^2) };
					 \addplot[shamrockgreen,  ultra thick, domain=0.6:1] {5*(1 - x)^2};

					 % N52
					 \addplot[deeppink,  ultra thick, domain=0.5:0.6] {20*x^2 - 20*x + 5 };
					 \addplot[deeppink,  ultra thick, domain=0.6:1] {-11.25*x^2 + 17.5*x - 6.25};

					 % N52
					 \addplot[burple,  ultra thick, domain=0.6:1] {6.25*x^2 - 7.5*x + 2.25};

					 \draw[ultra thin] (axis cs:\pgfkeysvalueof{/pgfplots/xmin},0) -- (axis cs:\pgfkeysvalueof{/pgfplots/xmax},0);
			 \end{axis}
		 \end{tikzpicture}
	 \end{center}
	#+end_export
** Representation
  - See[[http://jsxgraph.uni-bayreuth.de/wiki/index.php/B-splines][ this link]] (B-splines) and [[https://bl.ocks.org/jinroh/7524988][this]] link + [[https://www.youtube.com/watch?v=spUNpyF58BY][this video]] for a visual understanding of the different bases functions.
** Fitness function
*** What's the fitness?
	# Look at https://raw.githubusercontent.com/dfeich/org-babel-examples/master/beamer/beamer-example.org
	# for nice animations. PDF at https://github.com/dfeich/org-babel-examples/blob/master/beamer/beamer-example.pdf
	- <2-> Measure time at \( x(t) = x_B \)
***                                                                   :BMCOL:
	:PROPERTIES:
	:BEAMER_col: 0.3
	:END:
	#+begin_export latex
	\begin{center}
		\begin{tikzpicture}[scale=0.48]
		\begin{axis}[
			grid=major, % Display a grid
			grid style={dashed,gray!30}, % Set the style
			xlabel=$x$, % Set the labels
			% ylabel=$y$,
			ymin=-1.05,
			ymax=0.05
			]
			\node at (axis cs:0,0) [left] (a) {$A$};
			\node at (axis cs:1, -1) [right] (b) {$B$};
			% \draw [black, fill=black] (axis cs:0, 0) circle [radius=0.01];
			% \draw [black, fill=black] (axis cs:1,-1) circle [radius=0.01];
			\addplot[line width=2pt, metropolisorange, mark=none]
			% add a plot from table; you select the columns by using the actual name in
			% the .csv file (on top)
			table[col sep=comma] {data_from_optex/first_spline_profile.csv};
			\addplot[only marks, mark=*]
			table[col sep=comma] {data_from_optex/first_spline_time.csv};
		\end{axis}
		\end{tikzpicture}
	\end{center}
	#+end_export
	- <2->\( f = \SI{0.638}{\s}\)
***                                                                   :BMCOL:
	:PROPERTIES:
	:BEAMER_col: 0.3
	:END:
	#+begin_export latex
	\begin{center}
		\begin{tikzpicture}[scale=0.48]
		\begin{axis}[
			grid=major, % Display a grid
			grid style={dashed,gray!30}, % Set the style
			xlabel=$x$, % Set the labels
			ymin=-1.05,
			ymax=0.05
			]
			\node at (axis cs:0,0) [left] (a) {$A$};
			\node at (axis cs:1, -1) [right] (b) {$B$};
			% \draw [black, fill=black] (axis cs:0, 0) circle [radius=0.01];
			% \draw [black, fill=black] (axis cs:1,-1) circle [radius=0.01];
			\addplot[line width=2pt, royalblue, mark=none]
			% add a plot from table; you select the columns by using the actual name in
			% the .csv file (on top)
			table[col sep=comma] {data_from_optex/second_spline_profile.csv};
			\addplot[only marks, mark=*]
			table[col sep=comma] {data_from_optex/second_spline_time.csv};
		\end{axis}
		\end{tikzpicture}
	\end{center}
	#+end_export

	- <2->\( f = \SI{0.713}{\s}\)
***                                                                   :BMCOL:
	:PROPERTIES:
	:BEAMER_col: 0.3
	:END:
	#+begin_export latex
	\begin{center}
		\begin{tikzpicture}[scale=0.48]
		\begin{axis}[
			grid=major, % Display a grid
			grid style={dashed,gray!30}, % Set the style
			xlabel=$x$, % Set the labels
			ymin=-1.05,
			ymax=0.05
			]
			\node at (axis cs:0,0) [left] (a) {$A$};
			\node at (axis cs:1, -1) [right] (b) {$B$};
			% \draw [black, fill=black] (axis cs:0, 0) circle [radius=0.01];
			% \draw [black, fill=black] (axis cs:1,-1) circle [radius=0.01];
			\addplot[line width=2pt, scarlet, mark=none]
			% add a plot from table; you select the columns by using the actual name in
			% the .csv file (on top)
			table[col sep=comma] {data_from_optex/optimal_spline_profile.csv};
			\addplot[only marks, mark=*]
			table[col sep=comma] {data_from_optex/optimal_spline_time.csv};
		\end{axis}
		\end{tikzpicture}
	\end{center}
	#+end_export
	- <2->\( f = \SI{0.585}{\s}\)
** Constraints and penalties
*** Is unconstrained optimization a good idea?
	- <2-> *NO*! We penalize /bad/ solutions.
***                                                                   :BMCOL:
	:PROPERTIES:
	:BEAMER_col: 0.5
	:END:
	#+begin_export latex
	\begin{center}
		\begin{tikzpicture}[scale=0.65]
		\begin{axis}[
			grid=major, % Display a grid
			grid style={dashed,gray!30}, % Set the style
			xlabel=$x$, % Set the labels
			ylabel=$y$,
			ymin=-2.2,
			ymax=0.2
			]
			\node at (axis cs:0,0) [left] (a) {$A$};
			\node at (axis cs:1, -1) [right] (b) {$B$};
			% Different radii because its uneven
			\draw [black, fill=black] (axis cs:0, 0) circle [x radius=0.02, y radius=0.04];
			\draw [black, fill=black] (axis cs:1,-1) circle [x radius=0.02, y radius=0.04];
			\addplot[line width=2pt, scarlet, mark=none]
			% add a plot from table; you select the columns by using the actual name in
			% the .csv file (on top)
			table[col sep=comma] {data_from_optex/positive_slope_spline_profile.csv};
		\end{axis}
		\end{tikzpicture}
	\end{center}
	#+end_export
 	- <2-> Positive slope : simulation /fails/
***                                                                   :BMCOL:
	:PROPERTIES:
	:BEAMER_col: 0.5
	:END:
	#+begin_export latex
	\begin{center}
		\begin{tikzpicture}[scale=0.65]
		\begin{axis}[
			grid=major, % Display a grid
			grid style={dashed,gray!30}, % Set the style
			xlabel=$x$, % Set the labels
			ylabel=$y$,
			ymin=-2.2,
			ymax=0.2
			]
			\node at (axis cs:0,0) [left] (a) {$A$};
			\node at (axis cs:1, -1) [right] (b) {$B$};
			\draw [black, fill=black] (axis cs:0, 0) circle [x radius=0.02, y radius=0.04];
			\draw [black, fill=black] (axis cs:1,-1) circle [x radius=0.02, y radius=0.04];
			\addplot[line width=2pt, royalblue, mark=none]
			% add a plot from table; you select the columns by using the actual name in
			% the .csv file (on top)
			table[col sep=comma] {data_from_optex/third_spline_profile.csv};
			\only<2->{\draw[fill=metropolisorange, fill opacity=0.2] (axis cs:0, 0) rectangle (axis cs:1,-1.3)};
		\end{axis}
		\end{tikzpicture}
	\end{center}
	#+end_export
 	- <2-> Need realistic bounds on coefficients!
** Results
	[[file:~/code/optex/brachistochrone.mp4][Brachistochrone optimization]]
** Additional discussion
   Think about how these choices affect the optimization campaign?
*** Population size / number of generations
*** Number of spline parameters (aka the dimensionality of the problem)?
*** Penalization coefficients?
*** Optimize "part" of the problem?
*** Error tolerance of ODE solver?
** Aliters
*** Johann's solution                                         :B_block:BMCOL:
	:PROPERTIES:
	:BEAMER_col: 0.4
	:BEAMER_env: block
	:END:
	- Geometrical
	- Energy conservation
	- Shady (af).
*** Jakob Bernoulli's solution                                :B_block:BMCOL:
	:PROPERTIES:
	:BEAMER_col: 0.6
	:BEAMER_env: block
	:END:
	- Snell's law!
	- Led eventually to calculus of variations
*** Isaac Newton's solution                                         :B_block:
	:PROPERTIES:
	:BEAMER_env: block
	:END:
	- Minimal resistance problem
*** Calculus of variations / optimal control theory

	-  \( y^*  = \mathop{\mathrm{arg\,min}_y} L[y] \mathrel{\mathop:}=
       \displaystyle\int_{x_A}^{x_B} \dfrac{\sqrt{1 + (y'(x))^2}}{\sqrt{y(x)}}
      dx\)
*** The optimal solution is a cycloid!
** More history[fn:5]
  \footnotesize
  - Johann Bernoulli allowed six months for other solutions (apart from his and
    Jakob's)
  - At the request of Leibniz, the time was publicly extended for a year and a
    half.
  - At 4 p.m. on 29 January 1697 when he arrived home from the Royal Mint,
	Isaac Newton found the challenge in a letter from Johann Bernoulli.
  - Newton stayed up all night to solve it and mailed the solution anonymously
	by the next post
  - Upon reading the solution, Bernoulli recognized its author, exclaiming that
    he "recognizes a lion from his claw mark"
  - Johann had taken two weeks to solve the same problem
  - Newton also wrote, "I do not love to be dunned [pestered] and teased by
	foreigners about mathematical things..."
  - In the end, five mathematicians responded with solutions: Newton,
    Bernoulli(s), Leibniz, Tschirnhaus and l'Hôpital.
** Dido's isoperimetric problem[fn:6]
*** Constraints in the problem definition
	What is the closed curve which has the maximum area for a given perimeter?

	 #+begin_export latex
	 \begin{center}
		 \begin{tikzpicture}[scale=0.65]
		 \begin{axis}[axis equal,
			 grid=major, % Display a grid
			 grid style={dashed,gray!30}, % Set the style
			 xlabel=$x$, % Set the labels
			 ylabel=$f(x)$,
			 xmin=0,
			 xmax=1,
			 ymin=0,
			 ymax=0.6,
			 samples=100]
			 \addplot[royalblue,  line width=3pt, domain=0:1] {(0.25-(x-0.5)^2)^0.5};

			 \addplot[metropolisorange,  line width=3pt, domain=0:0.5] {2.8*(x-1.5*x^2)};
			 \addplot[metropolisorange,  line width=3pt, domain=0.5:1] {1.4*(1-x)^2};

			 \addplot[black,  line width=3pt, domain=0.0:0.5] {1.4*x^2};
			 \addplot[black,  line width=3pt, domain=0.5:1.0] {2.8*(-0.5-1.5*x^2+2*x)};

		 \end{axis}
		 \end{tikzpicture}
	 \end{center}
	 #+end_export
** Results
   - Constraint satsifaction by pre-processing and not by repair
   - [[file:~/code/optex/isoperimetric.mp4][Isoperimetric curve optimization]]

* Comparing CMA against GA
** CMAes vs GA--setup
*** Optimization on smooth functions
	- Two dimensional, \( C^{\infty} \) functions \( f(\gv{x}) \) : \( \left(\mathbb{R}^2,  \mathbb{R}, f, \leq \right) \)
	- shifted Schaffer function (optima in the middle well)
***                                                         :B_ignoreheading:
	:PROPERTIES:
	:BEAMER_env: ignoreheading
	:END:
	 #+begin_export latex
	\begin{center}
		\begin{tikzpicture}[
			declare function={schaffer=0.5 + ((sin(deg(sqrt(x^2+y^2))))^2-0.5)/(1+0.001*(x^2+y^2))^2;}, scale=0.9]
			\begin{axis}[
				colormap name=whitered,
				view={45}{65},
				enlargelimits=false,
				grid=major,
				domain=-8:8,
				y domain=-8:8,
				samples=41,
				xlabel=$x_1$,
				ylabel=$x_2$,
				zlabel={$f_{\text{schaffer}}$},
				colorbar,
			]
			\addplot3 [surf] {schaffer};
			\draw [black!50] (axis cs:-2.5,0,0) -- (axis cs:2.5,0,0);
			\draw [black!50] (axis cs:0,-2.5,0) -- (axis cs:0,2.5,0);
			\end{axis}
		\end{tikzpicture}
	\end{center}
	#+end_export
** CMAes vs GA--setup
***                                                         :B_ignoreheading:
	:PROPERTIES:
	:BEAMER_env: ignoreheading
	:END:
	- shifted Rastrigin function (optima in the middle well)
	 #+begin_export latex
	\begin{center}
		\begin{tikzpicture}[
			declare function={rastrigin=20 + (x^2 - 10*cos(deg(0.85*pi*x))) + (y^2 - 10*cos(deg(0.85*pi*y)));}, scale=1.0]
			\begin{axis}[
				colormap name=whitered,
				view={45}{65},
				enlargelimits=false,
				grid=major,
				domain=-8:8,
				y domain=-8:8,
				samples=41,
				xlabel=$x_1$,
				ylabel=$x_2$,
				zlabel={$f_{\text{rastrigin}}$},
				colorbar,
			]
			\addplot3 [surf] {rastrigin};
			\draw [black!50] (axis cs:-2.5,0,0) -- (axis cs:2.5,0,0);
			\draw [black!50] (axis cs:0,-2.5,0) -- (axis cs:0,2.5,0);
			\end{axis}
		\end{tikzpicture}
	\end{center}
	#+end_export
** Comparison between functions[fn:2]
***                                                                :B_column:
	:PROPERTIES:
	:BEAMER_env: column
	:BEAMER_col: 0.5
	:END:
#+CAPTION: Schaffer--setup
[[file:images/schaffer_start.png]]
***                                                                :B_column:
	:PROPERTIES:
	:BEAMER_env: column
	:BEAMER_col: 0.5
	:END:
#+CAPTION: Rastrigin--setup
[[file:images/rastrigin_start.png]]

*** Lighter region indicates smaller values                 :B_ignoreheading:
	:PROPERTIES:
	:BEAMER_env: ignoreheading
	:END:
	Lighter region indicates smaller values
** Simple ES
*** Scheme
	- *Sampling* : \( \gv{z}_i \sim \mathcal{N}\left( \gv{m}, \bv{C} \right) \)
	- *Mean-update* : \( \gv{m} \leftarrow \gv{z}_{1:\lambda} \)
	- *Covariance-update* : \( \bv{C} = \begin{bmatrix}\sigma^2_x & \sigma_x \sigma_y \\  \sigma_x
      \sigma_y & \sigma^2_y \end{bmatrix} \) \( \sigma_x, \sigma_y \) are fixed.
	- No other updates (on path etc.)
*** Legend
	- @@latex:{\color{shamrockgreen}@@Green@@latex:}@@ : Tracks the mean \(\gv{m}\).
	- @@latex:{\color{royalblue}@@Blue@@latex:}@@ : Tracks the sampled solutions
      at generetation \(g\).
	- @@latex:{\color{scarlet}@@Red@@latex:}@@ : Tracks the best individual so
      far.
*** Results
	Simple Evolution strategy from [[http://blog.otoro.net/2017/10/29/visual-evolution-strategies/][Otoro]] shown for 20 generations
** Simple ES-Observations
*** Convergence
	- What do you expect for general problems?
	-
*** Rate of convergence
	- Is this fast/slow convergence?
	-
*** Number of function evaluations?
	- High? Low? Not bad?
	-

** Simple ES-Observations
*** Convergence
	- What do you expect for general problems?
	- *Will get stuck--lack of diversity, keeps only best population* (See
      rastrigin, which temporarily gets stuck)
	- *Heavy* parameter dependence too
*** Rate of convergence
	- Is this fast/slow convergence?
	- *Slow--no history information*
*** Number of function evaluations?
	- High? Low? Not bad?
	- *Decent--but no promises for real life black-box optimization problems*

** Simple GA
*** Scheme
	- *Environmental selection* : Keep only best \( 10 \% \)
	- *Sampling* : Crossover from parents selected above with \( p_c = 1 \)
	- *Crossover* : Select two parents, obtain \(x\) or \(y\) from either parent
      with \( 0.5 \) probability (two coin tosses)
	- *Mutation* : Introduce Gaussian noise with fixed \( \sigma \)
	- No other updates (on path etc.)
*** Legend
	- @@latex:{\color{shamrockgreen}@@Green@@latex:}@@ : Tracks the elites from
      prior generation \(g\).
	- @@latex:{\color{royalblue}@@Blue@@latex:}@@ : Offsprings from candidate solutions.
	- @@latex:{\color{scarlet}@@Red@@latex:}@@ : Tracks the best individual so
      far.

** Simple GA-Observations
*** Convergence
	- What do you expect for general problems?
	-
*** Rate of convergence
	- Is this fast/slow convergence?
	-
*** Number of function evaluations?
	- High? Low? Not bad?
	-

** Simple GA-Observations
*** Convergence
	- What do you expect for general problems?
	- *Will get stuck--lack of diversity, keeps only elitist population*
	- *Heavy* parameter dependence
	- *Tracks* modality well (for both Schaffer and Rastrigin)
*** Rate of convergence
	- Is this fast/slow convergence?
	- *Slower* than simple ES
*** Number of function evaluations?
	- High? Low? Not bad?
	- *High*

** CMAes-Observations
*** Can you spot the updates?
	- \( \gv{m} \) update (fairly obvious)
	- Step size update
	  - Path update
	- Covariance matrix upfate
	  - Rank \( \mu \) updates
	  - Rank \( 1\) update (Path update)
***                                                                  :B_note:
	:PROPERTIES:
	:BEAMER_env: note
	:END:
	- Step size : Initially small, As soon as it sees all are moving in one
      direction, it quickly adapts in both cases.
	- Step size reduce. Rastrigin more difficult as
      optima close to zero, so step size reduce takes time.
	- Cov update : Direction is clearly important. First recognizes the ascent
      direction as a prinicpal component. Then of course the next one is
      complementary to it...
	- Most due to rank \( \mu \) update (as scaling applied there)
	- But rank one update also seen...

** CMAes-Observations
*** Convergence
	- What do you expect for general problems?
	- *Good* for problems of "moderate" dimensions
*** Rate of convergence
	- Is this fast/slow convergence?
	- *Fast* (Approximately brackets minima in \( \order{n} \) functional evaluations)
*** Number of function evaluations?
	- High? Low? Not bad?
	- Low (same as above)
** CMAes-Some interesting videos
	  - Mario https://www.youtube.com/watch?v=0iipyd7Gi70
	  - Rastrigin : https://www.youtube.com/watch?v=aP31Q7o2UGU
	  - Biped: https://www.youtube.com/watch?v=lOaWvOA9cb4
	  - Robot Invivo: https://www.youtube.com/watch?v=trR2Gc1tLzg
	  - Robot invitro: https://www.youtube.com/watch?v=fjTd06L-9bQ
	  - Knifefish : https://www.youtube.com/watch?v=3XjgZbs0t2g
	  - https://blog.openai.com/evolution-strategies/
***                                                                  :B_note:
	:PROPERTIES:
	:BEAMER_env: note
	:END:
	- Mario video : interesing solutions
	  - Why optimal? See what it is doing. It waits for mushroom bullet
	  - 0:35 complex paths
	  - Stupidity: at 0.02. IT does not wait. Maybe the user programmed for time
        to completion too.
	  - Stupidit at 0.14 too
	  - Waits for bullet at 0.18
	  - Waits for bllet at 0.29
	  - 0.35 is just awesome
	  - Waits for shroom at 0.40
	  - Waits for bullet at 0.45, 0.49
* Footnotes
[fn:6] [[https://mathematicalgarden.wordpress.com/2008/12/21/the-problem-of-dido/][Mathematical Garden]]

[fn:5]  [[https://en.wikipedia.org/wiki/Brachistochrone_curve][Brachistochrone curve wiki]]

[fn:3][[https://upload.wikimedia.org/wikipedia/commons/2/25/The_Normal_Distribution.svg][ Wikimedia Commons]]

[fn:4] [[https://www.slideshare.net/OsamaSalaheldin2/cmaes-presentation][CMAes overview, Slideshare]]

[fn:2] [[http://blog.otoro.net/2017/10/29/visual-evolution-strategies/][Otoro]]

[fn:1] https://math.stackexchange.com/q/2115701
